{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SM08: Custom Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classes\n",
    "\n",
    "When transforming data, often the predefined functions provided in libraries such as sklearn or category_encoders are only part of the transformation that needs to happen. This means creating custom transformers that can be incorporated into a sklearn pipeline (more on the sklearn pipeline later).\n",
    "\n",
    "For information on how to create a custom transformer, see the following tutorials:\n",
    "\n",
    "- [ML Data Pipelines with Custom Transformers in Python](https://towardsdatascience.com/custom-transformers-and-ml-data-pipelines-with-python-20ea2a7adb65)\n",
    "- [Creating custom scikit-learn Transformers](https://www.andrewvillazon.com/custom-scikit-learn-transformers/)\n",
    "- [Pipelines & Custom Transformers in scikit-learn: The step-by-step guide (with Python code)](https://towardsdatascience.com/pipelines-custom-transformers-in-scikit-learn-the-step-by-step-guide-with-python-code-4a7d9b068156)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneHotTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, feature_names):\n",
    "        self._feature_names = feature_names\n",
    "    \n",
    "    def fit(self, ori_df, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, ori_df, y=None):\n",
    "        print('Running OneHotTransformer')\n",
    "        df = ori_df[self._feature_names]\n",
    "        col_names = df.dropna(axis=1, how='all').columns\n",
    "        encoder = ce.OneHotEncoder(cols=col_names, use_cat_names=True, handle_missing='return_nan')\n",
    "        ce_one_hot = pd.DataFrame(encoder.fit_transform(df[col_names]), index=df.index)\n",
    "        ce_one_hot = ce_one_hot.astype(int)\n",
    "        df = ori_df.drop(self._feature_names, axis=1).merge(ce_one_hot, left_index=True, right_index=True, how='outer')\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create sklearn pipeline\n",
    "\n",
    "It might seem silly to use an sklearn pipeline when we're already creating a SageMaker pipeline. However, these pipelines do different things. \n",
    "\n",
    "The SageMaker pipeline controls how the data moves through the workflow, from data pull to transformation to training and evaluation to deployment.\n",
    "\n",
    "The sklearn pipeline strings together specific transformers and estimators to allow easy replication of data transformation for training purposes. The sklearn pipeline is particularly useful when incorporated into a `preprocessing.py` script because it can be exported as a joblib. This allows the same transformations to be done in both training and prediction, which makes it much easier to ensure the same code is applied in both places.\n",
    "\n",
    "For more information on sklearn pipelines see the following:\n",
    "\n",
    "- [6.1. Pipelines and composite estimators](https://scikit-learn.org/stable/modules/compose.html)\n",
    "- [sklearn.pipeline.Pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = Pipeline([\n",
    "    ('onehot', OneHotTransformer(cat_cols.keys()))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run script\n",
    "\n",
    "The final step is to run the code. When using the script as a transformer, estimator, etc, use `if __name__ == '__main__':` to initialize the code. Anything after this line will execute on the EC2.\n",
    "\n",
    "### Input/Output\n",
    "\n",
    "The `input_path` and `output_path` variables are unique to working with a SageMaker pipeline. By default SageMaker uses `input_path = '/opt/ml/processing/input'` and `output_path = '/opt/ml/processing/output'`. All this code really does is specify where data and files are located.\n",
    "\n",
    "In the pipeline step (covered next), the input and output locations are defined in the step. It looks something like this:\n",
    "\n",
    "```\n",
    "inputs=[\n",
    "        ProcessingInput(source=input_data, destination=\"/opt/ml/processing/input\"),\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(\n",
    "            output_name=\"clean\",\n",
    "            source=\"/opt/ml/processing/output\",\n",
    "            destination=Join(\n",
    "                on=\"/\",\n",
    "                values=[\n",
    "                    \"s3://{}\".format(bucket),\n",
    "                    prefix,\n",
    "                    'processed',\n",
    "                    \"clean\"\n",
    "                ]\n",
    "```\n",
    "\n",
    "This makes it easy to change the files/folders loaded to the EC2 that actually executes the python script. As well as where the output is saved. Think of this process as an automated version of having to add code that saves things to S3.\n",
    "\n",
    "When specifying input, if all the files in a single folder are needed, the entire folder can be referenced, which will load the folder and all files in it to the `input_path`. Just make sure the references for file location include the `input_path` + `folder_name` instead of just the `input_path`.\n",
    "\n",
    "*Note*, when saving things to subdirectories, those directories need to be created first.\n",
    "\n",
    "To use the same code between training and inference/prediction, simply save the pre-processed data (for use in training) and dump the sklearn pipeline as a joblib. In order to reference these later, they need to be saved to separate locations because they are used in different steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    input_path = '/opt/ml/processing/input'\n",
    "    output_path = '/opt/ml/processing/output'\n",
    "    \n",
    "    try:\n",
    "        os.makedirs(os.path.join(output_path, 'data'))\n",
    "        os.makedirs(os.path.join(output_path, 'encoder'))\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    print('Reading data')\n",
    "    df = pd.read_table(input_path, header=None)\n",
    "    print('Preprocessing data')\n",
    "    processed_df = pd.DataFrame(preprocessor.fit_transform(train_data))\n",
    "    print('Saving dataframe')\n",
    "    df.to_json(os.path.join(output_path, 'data', 'train_data.json'))\n",
    "    print('Saving joblib')\n",
    "    joblib.dump(preprocessor, os.path.join(output_path, 'encoder', 'preprocess.joblib'))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "- [Pipelines & Custom Transformers in Scikit-learn](https://towardsdatascience.com/pipelines-custom-transformers-in-scikit-learn-ef792bbb3260)\n",
    "- [Pipelines & Custom Transformers in scikit-learn: The step-by-step guide (with Python code)](https://towardsdatascience.com/pipelines-custom-transformers-in-scikit-learn-the-step-by-step-guide-with-python-code-4a7d9b068156)\n",
    "- [How to transform items using sklearn Pipeline?](https://stackoverflow.com/questions/33469633/how-to-transform-items-using-sklearn-pipeline)\n",
    "- [6. Dataset transformations](https://scikit-learn.org/stable/data_transforms.html)\n",
    "- [6.1. Pipelines and composite estimators](https://scikit-learn.org/stable/modules/compose.html#pipelines-and-composite-estimators)\n",
    "- [6.3. Preprocessing data](https://scikit-learn.org/stable/modules/preprocessing.html#preprocessing-data)\n",
    "- [6.3.8. Custom transformers](https://scikit-learn.org/stable/modules/preprocessing.html#custom-transformers)\n",
    "- [sklearn.preprocessing.OneHotEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html#sklearn-preprocessing-onehotencoder)\n",
    "- [Entry 20: Scikit-Learn Pipeline](https://julielinx.github.io/blog/20_sklearn_pipeline/)\n",
    "- [Entry 20 notebook - SciKit Learn Pipeline](https://github.com/julielinx/datascience_diaries/blob/master/02_model_eval/20a_nb_sklearn_pipeline.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
