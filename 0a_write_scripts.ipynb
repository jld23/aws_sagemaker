{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68f1bf7e-7e59-4c5a-a64c-aacf54a55b07",
   "metadata": {},
   "source": [
    "# Write Scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844c8ede-1f61-4b44-a24e-7553071e2748",
   "metadata": {},
   "source": [
    "# Create Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7bf650b-e44c-4357-901c-a012979eb472",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing create_feats.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile create_feats.py\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import random\n",
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--sample-size', type=str, dest='sample_size')\n",
    "parser.add_argument('--group', type=str, dest='group')\n",
    "args = parser.parse_args()\n",
    "sample_ct = int(args.sample_size)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    output_path = '/opt/ml/processing/output'\n",
    "    \n",
    "    try:\n",
    "        os.makedirs(os.path.join(output_path, \"data\"))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # single value observations\n",
    "    tf_col = []\n",
    "    onehot_col = []\n",
    "    float_col = []\n",
    "    drop_col = []\n",
    "    xrand_col = []\n",
    "    group_col = []\n",
    "\n",
    "    tf_vals = ['true', 'false', np.nan, '1', '0']\n",
    "    onehot_vals = [np.nan, 'red', 'orange', 'yellow', 'green', 'blue', 'purple']\n",
    "    float_vals = list(range(0,10)) + [x/10 for x in range(0, 100, 5)] +[np.nan]\n",
    "    drop_vals = [np.nan] + list(range(0,10))\n",
    "    xrand_vals = list(range(5))\n",
    "    group_vals = ['first', 'second', 'third']\n",
    "\n",
    "    col_list = zip([tf_col, onehot_col, float_col, drop_col, xrand_col, group_col],\n",
    "                   [tf_vals, onehot_vals, float_vals, drop_vals, xrand_vals, group_vals])\n",
    "\n",
    "    for col, vals in col_list:\n",
    "        for _ in range(sample_ct):\n",
    "            col.append(random.choice(vals))\n",
    "        \n",
    "    # date observations\n",
    "    date_col = []\n",
    "\n",
    "    for _ in range(sample_ct):\n",
    "        try:\n",
    "            date = datetime.date(2022, random.randint(1, 12), random.randint(1, 31))\n",
    "            date_col.append(date)\n",
    "        except ValueError:\n",
    "            date_col.append(np.nan)\n",
    "\n",
    "    # multivalue observations\n",
    "    nbr_vals = list(range(0,10))\n",
    "    str_vals = ['apple', 'orange', 'grape', 'pineapple', 'strawberry', 'blueberry', 'grapefruit', 'apple']\n",
    "\n",
    "    nunique_col = []\n",
    "\n",
    "    for _ in range(sample_ct):\n",
    "        val_size = random.randint(0,6)\n",
    "        if val_size < 1:\n",
    "            nunique_col.append(np.nan)\n",
    "        else:\n",
    "            if random.randint(0,10) < 5:\n",
    "                val_type = str_vals\n",
    "            else:\n",
    "                val_type = [str(x) for x in nbr_vals]\n",
    "            val = random.choices(val_type,k=val_size)\n",
    "            strified = ','.join(val)\n",
    "            nunique_col.append(strified)\n",
    "\n",
    "    descstat_col = []\n",
    "    max_col = []\n",
    "\n",
    "    nbrlst_cols = [descstat_col, max_col]\n",
    "\n",
    "    for col in nbrlst_cols:\n",
    "        for _ in range(sample_ct):\n",
    "            val_size = random.randint(0,6)\n",
    "            if val_size < 1:\n",
    "                col.append(np.nan)\n",
    "            else:\n",
    "                val_type = [str(x) for x in nbr_vals]\n",
    "                val = random.choices(val_type,k=val_size)\n",
    "                strified = ','.join(val)\n",
    "                col.append(strified)\n",
    "\n",
    "    multi_col = []\n",
    "\n",
    "    for _ in range(sample_ct):\n",
    "        val_size = random.randint(0,6)\n",
    "        if val_size < 1:\n",
    "            multi_col.append(np.nan)\n",
    "        else:\n",
    "            val = random.choices(str_vals, k=val_size)\n",
    "            strified = ','.join(val)\n",
    "            multi_col.append(strified)\n",
    "\n",
    "        # create dataframe\n",
    "    sample_df = pd.DataFrame({\n",
    "        'true_false':tf_col,\n",
    "        'one_hot':onehot_col,\n",
    "        'dates':date_col,\n",
    "        'floats':float_col,\n",
    "        'max_of_list':max_col,\n",
    "        'nunique_of_list':nunique_col,\n",
    "        'desc_stats':descstat_col,\n",
    "        'multi_label':multi_col,\n",
    "        'random_col':drop_col,\n",
    "        'other':xrand_col,\n",
    "        'group':group_col})\n",
    "    print(f'Dataset provided sample size: {sample_ct}')\n",
    "    print(f'Full dataframe shape: {sample_df.shape}')\n",
    "    filtered_df = sample_df[sample_df['group']==args.group]\n",
    "    print(f'Dataframe shape after being filtered by its group: {filtered_df.shape}')\n",
    "    filtered_df.to_parquet(os.path.join(output_path, 'feats.parquet'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09412779-9e58-48a5-b74d-0ac3256af618",
   "metadata": {},
   "source": [
    "# Create Ground Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0d28436-5282-4839-9a7c-f2b1c1066eba",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing create_gt.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile create_gt.py\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--sample-size', type=str, dest='sample_size')\n",
    "parser.add_argument('--target', type=str, dest='target_col')\n",
    "args = parser.parse_args()\n",
    "sample_ct = int(args.sample_size)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    output_path = '/opt/ml/processing/output'\n",
    "    \n",
    "    try:\n",
    "        os.makedirs(os.path.join(output_path, \"data\"))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # single value observations\n",
    "    target_col = []\n",
    "    target_vals = [0,1]\n",
    "\n",
    "    col_list = zip([target_col],\n",
    "                   [target_vals])\n",
    "\n",
    "    for col, vals in col_list:\n",
    "        for _ in range(sample_ct):\n",
    "            col.append(random.choice(vals))\n",
    "\n",
    "    gt_df = pd.DataFrame({\n",
    "        args.target_col:target_col})\n",
    "    print(f'Ground truth provided sample size: {sample_ct}')\n",
    "    print(f'Ground truth dataframe shape: {gt_df.shape}')\n",
    "    gt_df.to_parquet(os.path.join(output_path, 'gt.parquet'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a312b614-2a5a-405f-ab96-a5b58648d5ad",
   "metadata": {},
   "source": [
    "# Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "347c34c7-ae8a-4f7e-849f-e1c3b0eaf832",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting transformers.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile transformers.py\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, OneHotEncoder\n",
    "\n",
    "class TrueFalseTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self._col_names = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self._col_names = list(X.columns)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        print('Running TrueFalseTransformer')\n",
    "        X.fillna('-1', inplace=True)\n",
    "        X = X.replace({'true':'1', 'false':'0'})\n",
    "        X = X.apply(pd.to_numeric, args=('coerce',))\n",
    "        return X\n",
    "\n",
    "    def get_feature_names(self):\n",
    "        return self._col_names\n",
    "\n",
    "class OneHotTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self._filler = 'ml_empty'\n",
    "        self._col_names = None\n",
    "        self._encoder = None\n",
    "        self._transformer = None\n",
    "        self._transformed_feats = []\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self._col_names = X.dropna(axis=1, how='all').columns\n",
    "        X = X[self._col_names].fillna(self._filler)\n",
    "        self._encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "        self._transformer = self._encoder.fit(X)\n",
    "        self._transformed_feats = self._transformer.get_feature_names_out()\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        print('Running OneHotTransformer')\n",
    "        X = self._transformer.transform(X[self._col_names])\n",
    "        return X\n",
    "\n",
    "    def get_feature_names(self):\n",
    "        return list(self._transformed_feats)\n",
    "\n",
    "class DateTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self._col_names = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        print('Running DateTransformer')\n",
    "        temp_df = pd.DataFrame(index=X.index.copy())\n",
    "\n",
    "        for col in X.columns:\n",
    "            X[col] = pd.to_datetime(X[col])\n",
    "            temp_df[f'{col}-month'] = X[col].dt.month.astype(float)\n",
    "            temp_df[f'{col}-day_of_week'] = X[col].dt.dayofweek.astype(float)\n",
    "            temp_df[f'{col}-hour'] = X[col].dt.hour.astype(float)\n",
    "            temp_df[f'{col}-day_of_month'] = X[col].dt.day.astype(float)\n",
    "            temp_df[f'{col}-is_month_start'] = X[col].dt.is_month_start.astype(int)\n",
    "            temp_df[f'{col}-is_month_end'] = X[col].dt.is_month_end.astype(int)\n",
    "        self._col_names = list(temp_df.columns)\n",
    "        temp_df = temp_df.fillna(-1)\n",
    "        return temp_df\n",
    "\n",
    "    def get_feature_names(self):\n",
    "        return self._col_names\n",
    "\n",
    "class FloatTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self._col_names = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self._col_names = list(X.columns)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        print('Running FloatTransformer')\n",
    "        for col in self._col_names:\n",
    "            if X[col].dtype != 'float':\n",
    "                X[col] = X[col].astype(float)\n",
    "        X = X.fillna(-1.0)\n",
    "        return X\n",
    "\n",
    "    def get_feature_names(self):\n",
    "        return self._col_names\n",
    "\n",
    "class ListMaxTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self._col_names = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self._col_names = list(X.columns)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        print('Running ListMaxTransformer')\n",
    "        temp_df = pd.DataFrame(index=X.index)\n",
    "        for col in self._col_names:\n",
    "            if X[col].dtype == 'str':\n",
    "                X[col].fillna('-1', inplace=True)\n",
    "                X[col] = X[col].str.split(pat=',').apply(set).apply(list)\n",
    "            temp_series = X[col].explode()\n",
    "            temp_series = temp_series.replace({'true':'1', 'false':'0'}).fillna('-1').apply(pd.to_numeric, args=('coerce',))\n",
    "            temp_series = temp_series.groupby(temp_series.index).max()\n",
    "            temp_df = temp_df.merge(temp_series, left_index=True, right_index=True, how='outer')\n",
    "        temp_df = temp_df.fillna(0)\n",
    "        return temp_df\n",
    "\n",
    "    def get_feature_names(self):\n",
    "        return self._col_names\n",
    "\n",
    "class ListNuniqueTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self._col_names = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self._col_names = list(X.columns)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        print('Running ListNuniqueTransformer')\n",
    "        temp_df = pd.DataFrame(index=X.index)\n",
    "        for col in self._col_names:\n",
    "            if X[col].dtype == 'str':\n",
    "                X[col] = X[col].dropna().str.split(pat=',').apply(set).apply(list)\n",
    "            temp_series = X[col].explode()\n",
    "            temp_series = temp_series.groupby(temp_series.index).nunique()\n",
    "            temp_df = temp_df.merge(temp_series, left_index=True, right_index=True, how='outer')\n",
    "        temp_df = temp_df.fillna(0)\n",
    "        return temp_df\n",
    "\n",
    "    def get_feature_names(self):\n",
    "        return self._col_names\n",
    "\n",
    "class DescStatTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self._col_names = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        print('Running DescStatTransformer')\n",
    "        temp_df = pd.DataFrame(index=X.index)\n",
    "        for col in X.columns:\n",
    "            if X[col].dtype == 'str':\n",
    "                X[col].fillna('-1', inplace=True)\n",
    "                X[col] = X[col].str.split(pat=',').apply(set).apply(list)\n",
    "            temp_series = X[col].explode()\n",
    "            temp_series = temp_series.fillna('-1').apply(pd.to_numeric, args=('coerce',))\n",
    "            temp_series = temp_series.groupby(temp_series.index).agg(['min', 'max', 'mean', 'std', 'nunique'])\n",
    "            temp_series.columns = [f'{col}-{x}' for x in temp_series.columns]\n",
    "            temp_df = temp_df.merge(temp_series, left_index=True, right_index=True, how='outer')\n",
    "        temp_df = temp_df.fillna(0)\n",
    "        self._col_names = list(temp_df.columns)\n",
    "        return temp_df\n",
    "\n",
    "    def get_feature_names(self):\n",
    "        return self._col_names\n",
    "\n",
    "class MultilabelTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self._filler = 'ml_empty'\n",
    "        self._encoder = None\n",
    "        self._transformer = None\n",
    "        self._col_names = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X = X.fillna(self._filler).str.split(pat=',').apply(set).apply(list)\n",
    "        self._encoder = MultiLabelBinarizer()\n",
    "        self._encoder.fit(X)\n",
    "        self._col_names = [X.name + '__' + x for x in self._encoder.classes_]\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        print('Running MultilabelTransformer')\n",
    "        X = X.fillna(self._filler).str.split(pat=',').apply(set).apply(list)\n",
    "        trans_array = self._encoder.transform(X)\n",
    "        df = pd.DataFrame(trans_array, columns=self._col_names, index=X.index)        \n",
    "        return df\n",
    "\n",
    "    def get_feature_names(self):\n",
    "        return self._col_names\n",
    "    \n",
    "class DropSingleValueCols(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self._col_names = []\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        for col in X.columns:\n",
    "            if X[col].nunique() > 1:\n",
    "                self._col_names.append(col)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        print('Running DropSingleValueCols')\n",
    "        X = X[self._col_names]\n",
    "        return X\n",
    "    \n",
    "    def get_feature_names(self):\n",
    "        return self._col_names\n",
    "    \n",
    "class RemoveCollinearity(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self._corr_dict = {}\n",
    "        self._drop_cols = set()\n",
    "        self._col_names = []\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        drop_list = []\n",
    "        for i, col in enumerate(X.columns):\n",
    "            sliced_col = abs(X.iloc[i+1:, i])\n",
    "            corr_feats = sliced_col[sliced_col > .97].index.tolist()\n",
    "            if len(corr_feats) > 0:\n",
    "                self._corr_dict[col] = corr_feats\n",
    "                drop_list += corr_feats\n",
    "        self._drop_cols = set(drop_list)\n",
    "        self._col_names = list(set(X.columns) - self._drop_cols)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        print('Running RemoveCollinearity')\n",
    "        X = X[self._col_names]\n",
    "        return X\n",
    "    \n",
    "    def get_feature_names(self):\n",
    "        return self._col_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e5a879-9e89-4ff0-b7be-879d6253d0e7",
   "metadata": {},
   "source": [
    "# Preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4070ad11-1b3f-4d2b-93a7-d06d703e5730",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting processor_script.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile processor_script.py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import warnings\n",
    "import joblib\n",
    "import argparse\n",
    "from io import StringIO\n",
    "import sys\n",
    "\n",
    "sys.path.append('/opt/app/0_sample_version/davids_workflow/my_version/')\n",
    "\n",
    "from transformers import TrueFalseTransformer\n",
    "from transformers import OneHotTransformer\n",
    "from transformers import DateTransformer\n",
    "from transformers import FloatTransformer\n",
    "from transformers import ListMaxTransformer\n",
    "from transformers import ListNuniqueTransformer\n",
    "from transformers import DescStatTransformer\n",
    "from transformers import MultilabelTransformer\n",
    "from transformers import DropSingleValueCols\n",
    "from transformers import RemoveCollinearity\n",
    "\n",
    "from sagemaker_containers.beta.framework import (\n",
    "    encoders, worker)\n",
    "\n",
    "import sklearn\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import set_config\n",
    "\n",
    "warnings.simplefilter(\"once\")\n",
    "set_config(transform_output=\"pandas\")\n",
    "\n",
    "# print(\"This is a test and you passed\")\n",
    "\n",
    "true_false = ['true_false']\n",
    "one_hot = ['one_hot']\n",
    "date_cols = ['dates']\n",
    "float_cols = ['floats']\n",
    "max_of_list = ['max_of_list']\n",
    "count_unique = ['nunique_of_list']\n",
    "desc_stat_cols = ['desc_stats']\n",
    "list_to_labels = ['multi_label']\n",
    "drop_cols = ['random_col']\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--target', type=str, dest='target_col')\n",
    "    parser.add_argument('--train-size', type=str, dest='train_size', default='0.8')\n",
    "    parser.add_argument('--file-format', type=str, dest='file_format', default='csv')\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    train_size = float(args.train_size)\n",
    "    if train_size > 1:\n",
    "        train_size = train_size/100\n",
    "    \n",
    "    def splitTransform(df, transformer, target_col=args.target_col):\n",
    "        x = df.drop(target_col, axis=1)\n",
    "        y = df[[target_col]]\n",
    "        feats = transformer.transform(x)\n",
    "        return feats, y\n",
    "    \n",
    "    input_path = '/opt/ml/processing/input/data'\n",
    "    output_path = '/opt/ml/processing/output'\n",
    "    \n",
    "    try:\n",
    "        os.makedirs(os.path.join(output_path, \"train\"), exist_ok=True)\n",
    "        os.makedirs(os.path.join(output_path, \"validate\"), exist_ok=True)\n",
    "        os.makedirs(os.path.join(output_path, \"test\", 'feats'), exist_ok=True)\n",
    "        os.makedirs(os.path.join(output_path, \"test\", 'target'), exist_ok=True)\n",
    "        os.makedirs(os.path.join(output_path, \"encoder\"), exist_ok=True)\n",
    "        os.makedirs(os.path.join(output_path, \"encoder_cols\"), exist_ok=True)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "#     print('input_path:', os.listdir(input_path))\n",
    "#     print('output_path folders:', os.listdir(output_path))\n",
    "    \n",
    "#     print('Pandas version:', pd.__version__)\n",
    "#     print('Numpy version:', np.__version__)\n",
    "#     print('SKLearn version:', sklearn.__version__)\n",
    "\n",
    "    print('Loading data')\n",
    "    feats_df = pd.read_parquet(os.path.join(input_path, 'feats', 'feats.parquet'))\n",
    "    print(f'Feature data size: {feats_df.shape}')\n",
    "    gt_df = pd.read_parquet(os.path.join(input_path, 'gt', 'gt.parquet'))\n",
    "    print(f'Ground truth data size {gt_df.shape}')\n",
    "    print('Combining features and ground truth')\n",
    "    full_data = feats_df.merge(gt_df, right_index=True, left_index=True, how='inner')\n",
    "    print(f'Merged dataframe size: {full_data.shape}')\n",
    "    print(f'Set target as {args.target_col}')\n",
    "    \n",
    "    train_data, other = train_test_split(full_data, train_size=train_size, random_state=12, stratify=full_data[args.target_col])\n",
    "    test_data, validate_data = train_test_split(other, train_size=0.5, random_state=12, stratify=other[args.target_col])\n",
    "    \n",
    "    print('Train data size:', train_data.shape)\n",
    "    print('Validate data size:', validate_data.shape)\n",
    "    print('Test data size:', test_data.shape)\n",
    "\n",
    "    # Offload memory\n",
    "    feats_df = None\n",
    "    gt_df = None\n",
    "    full_data = None\n",
    "    \n",
    "    # print(train_data.head())\n",
    "        \n",
    "    preprocessor = ColumnTransformer([\n",
    "        ('truefalse', TrueFalseTransformer(), true_false),\n",
    "        ('onehot', OneHotTransformer(), one_hot),\n",
    "        ('dates', DateTransformer(), date_cols),\n",
    "        ('floats', FloatTransformer(), float_cols),\n",
    "        ('listmax', ListMaxTransformer(), max_of_list),\n",
    "        ('nunique', ListNuniqueTransformer(), count_unique),\n",
    "        ('descstats', DescStatTransformer(), desc_stat_cols),\n",
    "        ('multilabel', MultilabelTransformer(), 'multi_label')],\n",
    "        verbose_feature_names_out=False)\n",
    "\n",
    "    extras = Pipeline([\n",
    "        ('dropsingle', DropSingleValueCols()),\n",
    "        ('removemulticollinear', RemoveCollinearity())])\n",
    "    \n",
    "    processor = Pipeline([\n",
    "        ('preprocess', preprocessor),\n",
    "        ('additional', extras)])\n",
    "\n",
    "    print('Preprocessing data')\n",
    "    processor.fit(train_data.drop(args.target_col, axis=1))\n",
    "    \n",
    "    print('Transforming data')\n",
    "    train_feats, train_target = splitTransform(train_data, processor)\n",
    "    validate_feats, validate_target = splitTransform(validate_data, processor)\n",
    "    test_feats, test_target = splitTransform(test_data, processor)\n",
    "\n",
    "    feature_names = list(train_feats.columns)\n",
    "    print(\"Selected features are: {}\".format(feature_names))\n",
    "\n",
    "    print('Saving preprocessor and feature_name joblibs')\n",
    "    joblib.dump(processor, os.path.join(output_path, 'encoder', 'preprocessor.joblib'))\n",
    "    joblib.dump(feature_names, os.path.join(output_path, 'encoder_cols', 'feature_names.joblib'))\n",
    "    \n",
    "    print('Saving dataframes')\n",
    "    pd.concat([train_target, train_feats], axis=1).to_csv(os.path.join(output_path, 'train', 'train.csv'), index=False)\n",
    "    pd.concat([validate_target, validate_feats], axis=1).to_csv(os.path.join(output_path, 'validate', 'validate.csv'), index=False)\n",
    "    # test.to_csv(os.path.join(output_path, 'test', 'feats', 'test.csv'), index=False)\n",
    "    test_feats.to_csv(os.path.join(output_path, 'test', 'feats', 'test_x.csv'), index=False, header=False)\n",
    "    test_target.to_csv(os.path.join(output_path, 'test', 'target', 'test_y.csv'), index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c403cfe-5135-4b93-a6ac-27586684fd39",
   "metadata": {},
   "source": [
    "# Processor Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4228cf2d-476d-4042-9d94-dd4136d1e3bf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting processor_model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile processor_model.py\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install(package):\n",
    "    subprocess.check_call([sys.executable, \"-q\", \"-m\", \"pip\", \"install\", package])\n",
    "def upgrade(package):\n",
    "    subprocess.check_call([sys.executable, \"-q\", \"-m\", \"pip\", \"install\", \"-U\", package])\n",
    "\n",
    "upgrade('scikit-learn==1.2.2')\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "import argparse\n",
    "import csv\n",
    "import json\n",
    "import joblib\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "\n",
    "from transformers import TrueFalseTransformer\n",
    "from transformers import OneHotTransformer\n",
    "from transformers import DateTransformer\n",
    "from transformers import FloatTransformer\n",
    "from transformers import ListMaxTransformer\n",
    "from transformers import ListNuniqueTransformer\n",
    "from transformers import DescStatTransformer\n",
    "from transformers import MultilabelTransformer\n",
    "from transformers import DropSingleValueCols\n",
    "from transformers import RemoveCollinearity\n",
    "\n",
    "from sklearn import set_config\n",
    "\n",
    "from sagemaker_containers.beta.framework import(\n",
    "    content_types,\n",
    "    encoders,\n",
    "    env,\n",
    "    modules,\n",
    "    transformer,\n",
    "    worker)\n",
    "\n",
    "warnings.simplefilter(\"once\")\n",
    "set_config(transform_output=\"pandas\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--input-model', type=str, default=os.environ.get('SM_CHANNEL_INPUT_MODEL'))\n",
    "    parser.add_argument('--model_dir', type=str, default=os.environ.get('SM_MODEL_DIR'))\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    print('Args: {}'.format(args))\n",
    "    \n",
    "    print('Loading preprocessor')\n",
    "    preprocessor = joblib.load(os.path.join(args.input_model, 'preprocessor.joblib'))\n",
    "    \n",
    "    print('Saving models')\n",
    "    # This is done so that it is all put into a tar.gz file that can be used at inference\n",
    "    joblib.dump(preprocessor, os.path.join(args.model_dir, 'preprocessor.joblib'))\n",
    "\n",
    "def input_fn(input_data, content_type):\n",
    "    '''Parse input data payload\n",
    "    \n",
    "    Accepts csv, parquet, or json file types'''\n",
    "    \n",
    "    print('Running input data for transformation')\n",
    "    \n",
    "    if content_type == 'text/csv':\n",
    "        df = pd.read_csv(StringIO(input_data))\n",
    "        # df = pd.read_csv(input_data)\n",
    "    elif content_type == 'application/x-parquet':\n",
    "        df = pd.read_parquet(input_data)\n",
    "    elif content_type == 'application/json':\n",
    "        df = json.loads(input_data)\n",
    "        # df = pd.read_json(input_data)\n",
    "    else:\n",
    "        raise ValueError(\"{} not supported by script\".format(content_type))\n",
    "    print(df)\n",
    "    return df\n",
    "        \n",
    "def output_fn(prediction, accept):\n",
    "    '''Format prediction output.\n",
    "    \n",
    "    The default accept/content-type between containers for serial inference is JSON.\n",
    "    We also want to set the ContentType or mimetype as the same value as accept so the next\n",
    "    container can read the response payload correctly.\n",
    "    '''\n",
    "    \n",
    "    print(f'Running output function with accept type {accept}')\n",
    "    \n",
    "    if accept == 'application/json':\n",
    "        instances = []\n",
    "        for row in prediction.tolist():\n",
    "            instances.append({'features': row})\n",
    "            \n",
    "        json_output = {'instances': instances}\n",
    "        \n",
    "        return worker.Response(json.dumps(json_output), mimetype=accept)\n",
    "    elif accept == 'text/csv':\n",
    "        return worker.Response(encoders.encode(prediction, accept), mimetype=accept)\n",
    "    else:\n",
    "        raise RuntimeException(f'{accept} accept type is not supported by this script')\n",
    "        \n",
    "def predict_fn(input_data, preprocessor):\n",
    "    '''Preprocess input data\n",
    "    \n",
    "    The default predict_fn uses .predict(), but our model is a preprocessor\n",
    "    so we want to use .transform().\n",
    "    '''\n",
    "    \n",
    "    print('Preprocessing data')    \n",
    "    print('Input data shape at predict_fn: {}'.format(input_data.shape))\n",
    "    features = preprocessor.transform(input_data)\n",
    "    print(f'Data shape after prediction: {features.shape}')\n",
    "    print(features)\n",
    "    return features        \n",
    "    \n",
    "def model_fn(model_dir):\n",
    "    '''Deserialize fitted model'''\n",
    "    \n",
    "    print('Running model function')\n",
    "    preprocessor = joblib.load(os.path.join(model_dir, 'preprocessor.joblib'))\n",
    "    return preprocessor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bce37b8-1a0f-4233-bec8-00a63db04a6c",
   "metadata": {},
   "source": [
    "# Evaluation Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d4ecc44-4bb1-4311-960d-bd0b8523aef9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting evaluate.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile evaluate.py\n",
    "import json\n",
    "import os\n",
    "import logging\n",
    "import pathlib\n",
    "import pickle\n",
    "import tarfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import(\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    confusion_matrix,\n",
    "    roc_curve,\n",
    "#     mine\n",
    "    auc, \n",
    "    precision_recall_curve, \n",
    "    precision_score,\n",
    "    average_precision_score,\n",
    "    roc_auc_score,\n",
    "    log_loss,\n",
    "    f1_score,\n",
    "    recall_score,\n",
    "    roc_curve,\n",
    "    make_scorer,\n",
    "    balanced_accuracy_score,\n",
    "    cohen_kappa_score,\n",
    "    matthews_corrcoef,\n",
    "    fbeta_score)\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.addHandler(logging.StreamHandler())\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    y_pred_path = \"/opt/ml/processing/input/predictions/test_x.csv.out\"\n",
    "    y_true_path = \"/opt/ml/processing/input/true_labels/test_y.csv\"\n",
    "    predictions = pd.read_csv(y_pred_path, header=None)\n",
    "    y_test = pd.read_csv(y_true_path, header=None)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    conf_matrix = confusion_matrix(y_test, predictions)\n",
    "    pr_curve = precision_recall_curve(y_test, predictions)\n",
    "    precision = precision_score(y_test, predictions)\n",
    "    avg_precision = average_precision_score(y_test, predictions)\n",
    "    roc_auc = roc_auc_score(y_test, predictions)\n",
    "    f1 = f1_score(y_test, predictions)\n",
    "    recall = recall_score(y_test, predictions)\n",
    "    roc = roc_curve(y_test, predictions)\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, predictions)\n",
    "    informedness = balanced_accuracy_score(y_test, predictions, adjusted=True)\n",
    "    cohen_kappa = cohen_kappa_score(y_test, predictions)\n",
    "    matthews_coef = matthews_corrcoef(y_test, predictions)\n",
    "    fbeta = fbeta_score(y_test, predictions, beta=0.5)\n",
    "        \n",
    "    print(\"Accuracy: {}\".format(accuracy))\n",
    "    print(\"Confusion matrix: {}\".format(conf_matrix))\n",
    "    print(\"Precision Recall Curve: {}\".format(pr_curve))\n",
    "    print(\"Precision: {}\".format(precision))\n",
    "    print(\"Average Percision: {}\".format(avg_precision))\n",
    "    print(\"ROC AUC: {}\".format(roc_auc))\n",
    "    print(\"F1: {}\".format(f1))\n",
    "    print(\"Recall: {}\".format(recall))\n",
    "    print(\"ROC: {}\".format(roc))\n",
    "    print(\"informedness: {}\".format(informedness))\n",
    "    print(\"Cohen Kappa: {}\".format(cohen_kappa))\n",
    "    print(\"Mathews Correlation Coefficient: {}\".format(matthews_coef))\n",
    "    print(\"Fbeta: {}\".format(fbeta))\n",
    "    \n",
    "    try:\n",
    "        auc_score = auc(fpr, tpr)\n",
    "        print(\"AUC: {}\".format(auc_score))\n",
    "    except:\n",
    "        print(\"AUC doesn't work\")\n",
    "    try:\n",
    "        log_loss_score = log_loss(y_test, predictions)\n",
    "        print(\"Log loss: {}\".format(log_loss_score))\n",
    "    except:\n",
    "        print(\"log_loss doesn't work\")\n",
    "    \n",
    "    \n",
    "    report_dict = {\n",
    "        \"binary_classification_metrics\": {\n",
    "            # \"auc\": {\"value\":auc_score, \"standard_deviation\":\"NaN\"},\n",
    "            \"precision\": {\"value\":precision, \"standard_deviation\":\"NaN\"},\n",
    "            \"avg_percision\": {\"value\":avg_precision, \"standard_deviation\":\"NaN\"},\n",
    "            \"roc_auc\": {\"value\":roc_auc, \"standard_deviation\":\"NaN\"},\n",
    "            # \"log_loss\": {\"value\":log_loss_score, \"standard_deviation\":\"NaN\"},\n",
    "            \"f1\": {\"value\":f1, \"standard_deviation\":\"NaN\"},\n",
    "            \"recall\": {\"value\":recall, \"standard_deviation\":\"NaN\"},\n",
    "            \"informedness\": {\"value\":informedness, \"standard_deviation\":\"NaN\"},\n",
    "            \"cohen_kappa\": {\"value\":cohen_kappa, \"standard_deviation\":\"NaN\"},\n",
    "            \"mathews_coef\": {\"value\":matthews_coef, \"standard_deviation\":\"NaN\"},\n",
    "            \"fbeta\": {\"value\":fbeta, \"standard_deviation\":\"NaN\"},\n",
    "            \"accuracy\": {\"value\":accuracy, \"standard_deviation\":\"NaN\"},\n",
    "            # \"roc\": {\n",
    "            #     \"fpr\": roc[0].tolist(),\n",
    "            #     \"tpr\": roc[1].tolist(),\n",
    "            #     \"thresholds\": roc[2].tolist()}\n",
    "            # \"pr_curve\": {\"precision\": pr_curve[0].tolist(),\n",
    "            #              \"recall\": pr_curve[1].tolist(),\n",
    "            #              \"thresholds\": pr_curve[2].tolist()},\n",
    "            \"confusion_matrix\": {\"0\": {\"0\": int(conf_matrix[0][0]), \"1\": int(conf_matrix[0][1])},\n",
    "                                 \"1\": {\"0\": int(conf_matrix[1][0]), \"1\": int(conf_matrix[1][1])}\n",
    "                                },\n",
    "            # \"receiver_operating_charastic_curve\": {\n",
    "            #     \"false_positive_rates\": list(fpr),\n",
    "            #     \"true_positive_rates\": list(tpr)\n",
    "            # }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print(report_dict)\n",
    "    output_dir = \"/opt/ml/processing/evaluation\"\n",
    "    pathlib.Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    evaluation_path = os.path.join(output_dir, 'evaluation.json')\n",
    "    with open(evaluation_path, \"w\") as f:\n",
    "        f.write(json.dumps(report_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4696b449-2410-48fa-8407-ae366e0922ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
