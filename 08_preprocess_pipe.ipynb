{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SM08: Preprocessing Script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code to preprocess the [Insurance Company Benchmark (COIL 2000) dataset](https://archive.ics.uci.edu/ml/datasets/Insurance+Company+Benchmark+%28COIL+2000%29) was developed in posts [SM07](). This notebook will turn that code into the script for the pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update EC2 instance\n",
    "\n",
    "Writing the `preprocess.py` script is very similar to writing the `etl.py` script. The major difference is that I want to use a library that isn't installed by default and want to ensure package versions for several libraries. To do this I need to consider the pre-built EC2 instance configurations vs other options.\n",
    "\n",
    "AWS provides several different pre-built EC2 instance configurations. Unfortunately, there's always one package that needs to be updated to a specific version or isn't included by default. AWS generally recommends the following two solutions:\n",
    "\n",
    "- Use a `requirements.txt` (only available for estimator instances, not processor instances)\n",
    "- Create a custom EC2 image, load it to ECR (elastic container registry), and reference it in your pipeline\n",
    "\n",
    "When first starting out, we don't want to have to figure out how to convert a transformer to an estimator. We just want to be able to run the python script and save the outputs to a designated S3 location. So, the `requirements.txt` is out. For information on how to do it, see the [Using thrid-party libraries](https://sagemaker.readthedocs.io/en/stable/frameworks/sklearn/using_sklearn.html#using-third-party-libraries) section in the documentation.\n",
    "\n",
    "The directions to create a custom EC2 image generally involve going into another system, such as the AWS CLI. Our goal is to keep as much together in SageMaker as humanly possible. This rules out creating our own image. For information on how to create an image and load it to ECR, see the [Building your own algorithm container](https://sagemaker-examples.readthedocs.io/en/latest/advanced_functionality/scikit_bring_your_own/scikit_bring_your_own.html#Building-your-own-algorithm-container) section of the documentation or [Pushing a Docker image](https://docs.aws.amazon.com/AmazonECR/latest/userguide/docker-push-ecr-image.html) in the ECR User Guide. *Fair warning*, it isn't recommended to create a docker container from within a docker container (which is what SageMaker Studio is). To create the container, you'll need to use the AWS CLI or a SageMaker instance (not Studio).\n",
    "\n",
    "Stackoverflow to the rescue. [This answer](https://stackoverflow.com/a/63925135) gave us the information we needed to simply install or update the specific packages we needed. The code is included directly in the python script and is easy to use. We update the code to be able to load or upgrade a package as necessary.\n",
    "\n",
    "If we get to the point that we frequently need a specific configuration, we'll want to further explore creating our own image to upload to ECR.\n",
    "\n",
    "The code to install or upgrade a package on the EC2 is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def install(package):\n",
    "    subprocess.check_call([sys.executable, \"-q\", \"-m\", \"pip\", \"install\", package])\n",
    "def upgrade(package):\n",
    "    subprocess.check_call([sys.executable, \"-q\", \"-m\", \"pip\", \"install\", package, '--upgrade'])\n",
    "    \n",
    "upgrade('pandas==1.3.5')\n",
    "upgrade('numpy')\n",
    "install('category_encoders')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create directories\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    try:\n",
    "        os.makedirs(os.path.join(output_path, \"train\"))\n",
    "        os.makedirs(os.path.join(output_path, \"validate\"))\n",
    "        os.makedirs(os.path.join(output_path, \"test\"))\n",
    "        os.makedirs(os.path.join(output_path, 'encoder'))\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    train_data, validation_data, test_data = np.split(\n",
    "        processed_df.sample(frac=1, random_state=1729),\n",
    "        [int(0.7 * len(processed_df)), int(0.9 * len(processed_df))],)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save encoder\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    joblib.dump(encoder, os.path.join(output_path, 'encoder', encoder_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write Script\n",
    "\n",
    "Put it all together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting preprocess.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile preprocess.py\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install(package):\n",
    "    subprocess.check_call([sys.executable, \"-q\", \"-m\", \"pip\", \"install\", package])\n",
    "def upgrade(package):\n",
    "    subprocess.check_call([sys.executable, \"-q\", \"-m\", \"pip\", \"install\", package, '--upgrade'])\n",
    "    \n",
    "upgrade('pandas==1.3.5')\n",
    "upgrade('numpy')\n",
    "install('category_encoders')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import category_encoders as ce\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    input_path = '/opt/ml/processing/input'\n",
    "    output_path = '/opt/ml/processing/output'\n",
    " \n",
    "    try:\n",
    "        os.makedirs(os.path.join(output_path, \"train\"))\n",
    "        os.makedirs(os.path.join(output_path, \"validate\"))\n",
    "        os.makedirs(os.path.join(output_path, \"test\"))\n",
    "        os.makedirs(os.path.join(output_path, 'encoder'))\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    cat_cols = ['zip_agg Customer Subtype', 'zip_agg Customer main type']\n",
    "\n",
    "    df = pd.read_csv(os.path.join(input_path, 'full_data.csv'))\n",
    "    print('Preprocessing data')\n",
    "    encoder = ce.OneHotEncoder(cols=cat_cols, use_cat_names=True, handle_missing='return_nan')\n",
    "    processed_df = encoder.fit_transform(df)\n",
    "\n",
    "    train_data, validation_data, test_data = np.split(\n",
    "        processed_df.sample(frac=1, random_state=1729),\n",
    "        [int(0.7 * len(processed_df)), int(0.9 * len(processed_df))],)\n",
    "    \n",
    "    print('Saving dataframe')\n",
    "    train_data.to_csv(os.path.join(output_path, 'train', 'train_feats.csv'))\n",
    "    validation_data.to_csv(os.path.join(output_path, 'validate', 'validate_feats.csv'))\n",
    "    test_data.to_csv(os.path.join(output_path, 'test', 'test_feats.csv'))\n",
    "                              \n",
    "    print('Saving preprocessor joblib')\n",
    "    encoder_name = 'preprocessor.joblib'\n",
    "    joblib.dump(encoder, os.path.join(output_path, 'encoder', encoder_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write Pipeline\n",
    "\n",
    "Write pipeline to run the `.py` script.\n",
    "\n",
    "Foundations of pipelines is in [SM03](). Only new thing here is the multiple outputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The input argument instance_type of function (sagemaker.image_uris.retrieve) is a pipeline variable (<class 'sagemaker.workflow.parameters.ParameterString'>), which is not allowed. The default_value of this Parameter object will be used to override it. Please make sure the default_value is valid.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "_PipelineExecution(arn='arn:aws:sagemaker:us-east-1:707031497630:pipeline/insexample/execution/5f2oz1vy9ub6', sagemaker_session=<sagemaker.session.Session object at 0x7fc80031b0d0>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sagemaker\n",
    "import sagemaker.session\n",
    "\n",
    "from sagemaker.workflow.parameters import (\n",
    "    ParameterInteger,\n",
    "    ParameterString,\n",
    ")\n",
    "\n",
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.workflow.steps import ProcessingStep\n",
    "from sagemaker.workflow.functions import Join\n",
    "from sagemaker.workflow.execution_variables import ExecutionVariables\n",
    "\n",
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "\n",
    "session = sagemaker.session.Session()\n",
    "region = session.boto_region_name\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "bucket = session.default_bucket()\n",
    "prefix = '1_ins_dataset'\n",
    "pipeline_name = \"InsExample\"  # SageMaker Pipeline name\n",
    "model_package_group_name = \"Insurance Co Example\"  # Model name in model registry\n",
    "framework_version = \"0.23-1\"\n",
    "\n",
    "input_uri = f's3://{bucket}/{prefix}/clean/full_data.csv'\n",
    "\n",
    "tags = [\n",
    "    {\"Key\": \"PLATFORM\", \"Value\": \"FO-ML\"},\n",
    "    {\"Key\": \"BUSINESS_REGION\", \"Value\": \"GLOBAL\"},\n",
    "    {\"Key\": \"BUSINESS_UNIT\", \"Value\": \"MOBILITY\"},\n",
    "    {\"Key\": \"CLIENT\", \"Value\": \"MULTI_TENANT\"}\n",
    "   ]\n",
    "\n",
    "# tags = [\n",
    "#     {\"Key\": \"DATASET\", \"Value\": \"InsCOIL\"},\n",
    "#     {\"Key\": \"SOURCE\", \"Value\": \"UCI\"}\n",
    "#    ]\n",
    "\n",
    "processing_instance_count = ParameterInteger(name=\"ProcessingInstanceCount\", default_value=1)\n",
    "\n",
    "processing_instance_type = ParameterString(\n",
    "    name=\"ProcessingInstanceType\", default_value=\"ml.t3.medium\")\n",
    "    \n",
    "input_data = ParameterString(\n",
    "    name=\"InputData\",\n",
    "    default_value=input_uri\n",
    ")\n",
    "\n",
    "sklearn_processor = SKLearnProcessor(\n",
    "    framework_version=framework_version,\n",
    "    role=role,\n",
    "    instance_type=processing_instance_type,\n",
    "    instance_count=processing_instance_count,\n",
    "    base_job_name=\"ins-example-job\"\n",
    ")\n",
    "\n",
    "step_preprocess = ProcessingStep(\n",
    "    name=\"preprocess_data\",\n",
    "    processor=sklearn_processor,\n",
    "    inputs=[\n",
    "        ProcessingInput(source=input_data, destination=\"/opt/ml/processing/input\")\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(\n",
    "            output_name=\"train\",\n",
    "            source=\"/opt/ml/processing/output/train\",\n",
    "            destination=Join(\n",
    "                on=\"/\",\n",
    "                values=[\n",
    "                    \"s3://{}\".format(bucket),\n",
    "                    prefix,\n",
    "                    'final',\n",
    "                    \"train\"\n",
    "                ],\n",
    "            ),\n",
    "        ),\n",
    "        ProcessingOutput(\n",
    "            output_name=\"validate\",\n",
    "            source=\"/opt/ml/processing/output/validate\",\n",
    "            destination=Join(\n",
    "                on=\"/\",\n",
    "                values=[\n",
    "                    \"s3://{}\".format(bucket),\n",
    "                    prefix,\n",
    "                    'final',\n",
    "                    \"validate\"\n",
    "                ],\n",
    "            ),\n",
    "        ),\n",
    "        ProcessingOutput(\n",
    "            output_name=\"test\",\n",
    "            source=\"/opt/ml/processing/output/test\",\n",
    "            destination=Join(\n",
    "                on=\"/\",\n",
    "                values=[\n",
    "                    \"s3://{}\".format(bucket),\n",
    "                    prefix,\n",
    "                    'final',\n",
    "                    \"test\"\n",
    "                ],\n",
    "            ),\n",
    "        ),\n",
    "        ProcessingOutput(\n",
    "            output_name=\"encoder\",\n",
    "            source=\"/opt/ml/processing/output/encoder\",\n",
    "            destination=Join(\n",
    "                on=\"/\",\n",
    "                values=[\n",
    "                    \"s3://{}\".format(bucket),\n",
    "                    prefix,\n",
    "                    'final',\n",
    "                    'encoder'\n",
    "                ],\n",
    "            ),\n",
    "        ),\n",
    "    ],\n",
    "    code=\"preprocess.py\"\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    name=pipeline_name,\n",
    "    parameters=[\n",
    "        processing_instance_type,\n",
    "        processing_instance_count,\n",
    "        input_data,\n",
    "    ],\n",
    "    steps=[step_preprocess])\n",
    "\n",
    "pipeline.upsert(role_arn=role, tags=tags)\n",
    "\n",
    "pipeline.start(execution_display_name=\"InsPreprocess4\")"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
