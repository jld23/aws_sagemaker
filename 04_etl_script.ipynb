{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SM04: ETL Script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bulk of the code to clean the [Insurance Company Benchmark (COIL 2000) dataset](https://archive.ics.uci.edu/ml/datasets/Insurance+Company+Benchmark+%28COIL+2000%29) was developed in posts [SM01](https://julielinx.github.io/blog/aws01_read_from_s3/) and [SM02](https://julielinx.github.io/blog/aws02_clean_data/). Now that I've covered how the pipeline works, I can tackle streamlining my data cleaning code into a single Python script."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write Python script\n",
    "\n",
    "The first step is to create the `.py` script that will be run by the pipeline step. I pretty much already wrote this code in the first two posts. The only difference between the `.py` script below and those posts is that I don't have to save intermediate steps to S3. I do all the processing at one time.\n",
    "\n",
    "I'll only walk through the code specific to being able to run the code in the pipeline. The code that does the actual work of cleaning the data was already explained in detail in posts [SM01]() and [SM02]()."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create `.py` file\n",
    "\n",
    "I prefer to be able to do all of my work within the same interface. I have nothing against working in an IDE like Visual Studio or PyCharm, I just don't like switching between interfaces in the middle of a project and trying to keep which files need to be opened where straight and synced.\n",
    "\n",
    "While a `.py` file type isn't available to create under the `Launch` options in SageMaker, I can get around this using the `%%writefile` magic function. This magic function writes whatever file I want to the current working directory. This trick also works if I need to manually create `.txt` or other files types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile etl.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries\n",
    "\n",
    "As always, I must import libraries. This pretty much always comes at the beginning of a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define parameters and functions\n",
    "\n",
    "The second part to most scripts is parameter and function definitions. I generally put my parameters first because they're the thing I update most regularly. Putting them first makes them easy to find and update. I'm especially prone to updating parameters while prototyping. I'll hard code in parameters, then make them more dynamic once the code works.\n",
    "\n",
    "Functions come second because I generally leave these alone once I know they work. However, I want them all in the same place so they're easy to find, reference, and update when necessary. I frequently need to reference my functions to determine expected input and output.\n",
    "\n",
    "This particular script doesn't need any custom functions, but if I pull in the function from the SM02 post, it would look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = '1_ins_dataset'\n",
    "\n",
    "def read_mult_txt(bucket, prefix):\n",
    "    s3_resource = boto3.resource(\"s3\")\n",
    "    s3_bucket = s3_resource.Bucket(bucket)\n",
    "\n",
    "    files = {}\n",
    "    for object_summary in s3_bucket.objects.filter(Prefix=prefix):\n",
    "        if (len(object_summary.key.rsplit('.')) == 2) & (len(object_summary.key.split('/')) <= 3):\n",
    "            files[object_summary.key.split('/')[-1].split('.')[0]] = f\"s3://{bucket}/{object_summary.key}\"\n",
    "            \n",
    "    df_dict = {}\n",
    "    for df_name in files.keys():\n",
    "        df_dict[df_name] = pd.read_csv(files[df_name])\n",
    "\n",
    "    return df_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `__name__ == '__main__'`\n",
    "\n",
    "To be honest, when working in Jupyter Notebooks, I never bother with `if __name__ == '__main__':`. I just run the code cell by cell until I get what I need. However, when running a `.py` script as a step in a SageMaker Pipeline, the code needs to be treated as a more formal piece of stand alone code.\n",
    "\n",
    "Anything that should be run when the script is called goes under `if __name__ == '__main__':`. Don't forget to indent everything under it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define filepaths\n",
    "\n",
    "Coming from a background working almost exclusively in Jupyter Notebooks, the next step confused me at first.\n",
    "\n",
    "In Jupyter Notebooks, I read/write directly from/to S3. Locally, my read/writes go into the current working directory or somewhere within the folder structure that I designate (still generally based on the working directory instead of full filepaths to allow easy handling between different users). \n",
    "\n",
    "When working with SageMaker Pipelines, the filepath(s) to S3 is defined in the pipeline step, not in the `.py` script. The pipeline deposits the file(s) specified in the pipeline step into the instance it spins up to run the `.py` code. SageMaker deposits the file(s) into specific folders within that instance.\n",
    "\n",
    "The step type used to clean data is a Processing step. This step type uses the default folder locations `/opt/ml/processing/input` and `/opt/ml/processing/output`.\n",
    "\n",
    "The input location is where the file(s) designated in the Pipeline step are deposited. Thus any inputs I need are referenced from that filepath, not the working directory of the `.py` script.\n",
    "\n",
    "The output location is where SageMaker expects any produced file(s) to be located.\n",
    "\n",
    "#### Example:\n",
    "\n",
    "**Input**: In post SM01, I read files directly from the internet. As these are `.txt` files and my code is prepared to read them in this way, I can specify the URLs in the Pipeline step and they'll be deposited into the `/opt/ml/processing/input` folder. From a production stand point, this means I can create a Pipeline and specify files in different locations without having to change the Pipeline itself. I'll cover this in more detail in a later post.\n",
    "\n",
    "**Output**: The final product of my code is a single dataframe saved as a `.csv`. Placing this file in the `/opt/ml/processing/output` folder allows me to reference it in my Pipeline step and designate where to save it, generally an S3 location. I can also reference it in subsequent steps, allowing me to easily pass files from one step to the next. Additionally, using this method tells SageMaker what steps to run in what order. More on that in later posts.\n",
    "\n",
    "Why `/opt/ml/processing/input` and `/opt/ml/processing/output`? It's just what AWS decided to call the folders. I can easily create my own directories if I feel like it, I just need to use the `os.makedirs()` function and put the newly created filepath in the `input` or `output` parameter. I'll need this functionality later, but for now, why complicate things? The defaults are perfectly servicable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    input_path = '/opt/ml/processing/input'\n",
    "    output_path = '/opt/ml/processing/output'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final script\n",
    "\n",
    "The final Python script looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting etl.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile etl.py\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    input_path = '/opt/ml/processing/input'\n",
    "    output_path = '/opt/ml/processing/output'\n",
    "    \n",
    "    col_names = ['zip_agg_customer_subtype',\n",
    "                 'zip_agg_number_of_houses',\n",
    "                 'zip_agg_avg_size_household',\n",
    "                 'zip_agg_avg_age',\n",
    "                 'zip_agg_customer_main_type',\n",
    "                 'zip_agg_roman_catholic',\n",
    "                 'zip_agg_protestant',\n",
    "                 'zip_agg_other_religion',\n",
    "                 'zip_agg_no_religion',\n",
    "                 'zip_agg_married',\n",
    "                 'zip_agg_living_together',\n",
    "                 'zip_agg_other_relation',\n",
    "                 'zip_agg_singles',\n",
    "                 'zip_agg_household_without_children',\n",
    "                 'zip_agg_household_with_children',\n",
    "                 'zip_agg_high_level_education',\n",
    "                 'zip_agg_medium_level_education',\n",
    "                 'zip_agg_lower_level_education',\n",
    "                 'zip_agg_high_status',\n",
    "                 'zip_agg_entrepreneur',\n",
    "                 'zip_agg_farmer',\n",
    "                 'zip_agg_middle_management',\n",
    "                 'zip_agg_skilled_labourers',\n",
    "                 'zip_agg_unskilled_labourers',\n",
    "                 'zip_agg_social_class_a',\n",
    "                 'zip_agg_social_class_b1',\n",
    "                 'zip_agg_social_class_b2',\n",
    "                 'zip_agg_social_class_c',\n",
    "                 'zip_agg_social_class_d',\n",
    "                 'zip_agg_rented_house',\n",
    "                 'zip_agg_home_owners',\n",
    "                 'zip_agg_1_car',\n",
    "                 'zip_agg_2_cars',\n",
    "                 'zip_agg_no_car',\n",
    "                 'zip_agg_national_health_service',\n",
    "                 'zip_agg_private_health_insurance',\n",
    "                 'zip_agg_income_<_30.000',\n",
    "                 'zip_agg_income_30-45.000',\n",
    "                 'zip_agg_income_45-75.000',\n",
    "                 'zip_agg_income_75-122.000',\n",
    "                 'zip_agg_income_>123.000',\n",
    "                 'zip_agg_average_income',\n",
    "                 'zip_agg_purchasing_power_class',\n",
    "                 'contri_private_third_party_ins',\n",
    "                 'contri_third_party_ins_(firms)',\n",
    "                 'contri_third_party_ins_(agriculture)',\n",
    "                 'contri_car_policies',\n",
    "                 'contri_delivery_van_policies',\n",
    "                 'contri_motorcycle/scooter_policies',\n",
    "                 'contri_lorry_policies',\n",
    "                 'contri_trailer_policies',\n",
    "                 'contri_tractor_policies',\n",
    "                 'contri_agricultural_machines_policies',\n",
    "                 'contri_moped_policies',\n",
    "                 'contri_life_ins',\n",
    "                 'contri_private_accident_ins_policies',\n",
    "                 'contri_family_accidents_ins_policies',\n",
    "                 'contri_disability_ins_policies',\n",
    "                 'contri_fire_policies',\n",
    "                 'contri_surfboard_policies',\n",
    "                 'contri_boat_policies',\n",
    "                 'contri_bicycle_policies',\n",
    "                 'contri_property_ins_policies',\n",
    "                 'contri_ss_ins_policies',\n",
    "                 'nbr_private_third_party_ins',\n",
    "                 'nbr_third_party_ins_(firms)',\n",
    "                 'nbr_third_party_ins_(agriculture)',\n",
    "                 'nbr_car_policies',\n",
    "                 'nbr_delivery_van_policies',\n",
    "                 'nbr_motorcycle/scooter_policies',\n",
    "                 'nbr_lorry_policies',\n",
    "                 'nbr_trailer_policies',\n",
    "                 'nbr_tractor_policies',\n",
    "                 'nbr_agricultural_machines_policies',\n",
    "                 'nbr_moped_policies',\n",
    "                 'nbr_life_ins',\n",
    "                 'nbr_private_accident_ins_policies',\n",
    "                 'nbr_family_accidents_ins_policies',\n",
    "                 'nbr_disability_ins_policies',\n",
    "                 'nbr_fire_policies',\n",
    "                 'nbr_surfboard_policies',\n",
    "                 'nbr_boat_policies',\n",
    "                 'nbr_bicycle_policies',\n",
    "                 'nbr_property_ins_policies',\n",
    "                 'nbr_ss_ins_policies',\n",
    "                 'nbr_mobile_home_policies']\n",
    "\n",
    "    train = pd.read_csv(os.path.join(input_path, 'train.csv'))\n",
    "    test = pd.read_csv(os.path.join(input_path, 'test.csv'))\n",
    "    ground_truth = pd.read_csv(os.path.join(input_path, 'gt.csv'))\n",
    "    columns = pd.read_csv(os.path.join(input_path, 'col_info.csv'))\n",
    "\n",
    "    data_dict = {}\n",
    "    data_dict['feat_info'] = columns.iloc[1:87, 0].str.split(n=2, expand=True)\n",
    "    data_dict['feat_info'].columns = columns.iloc[0, 0].split(maxsplit=2)\n",
    "    data_dict['L0'] = columns.iloc[89:130, 0].str.split(n=1, expand=True)\n",
    "    data_dict['L0'].columns = columns.iloc[88, 0].split()\n",
    "    data_dict['L2'] = columns.iloc[138:148, 0].str.split(n=1, expand=True)\n",
    "    data_dict['L2'].columns = ['Value', 'Bin']\n",
    "\n",
    "    test_df = pd.concat([test, ground_truth], axis=1)\n",
    "    test_df.columns = data_dict['feat_info']['Name'].to_list()\n",
    "    train.columns = data_dict['feat_info']['Name'].to_list()\n",
    "\n",
    "    df = pd.concat([train, test_df], ignore_index=True)\n",
    "    df.columns = col_names\n",
    "\n",
    "    data_dict['L0']['Value'] = pd.to_numeric(data_dict['L0']['Value'])\n",
    "    l0_dict = data_dict['L0'].set_index('Value').to_dict()['Label']\n",
    "    data_dict['L2']['Value'] = pd.to_numeric(data_dict['L2']['Value'])\n",
    "    l2_dict = data_dict['L2'].set_index('Value').to_dict()['Bin']\n",
    "    df[df.columns[0]] = df[df.columns[0]].replace(l0_dict)\n",
    "    df[df.columns[4]] = df[df.columns[4]].replace(l2_dict)\n",
    "\n",
    "    df.to_csv(os.path.join(output_path, 'full_data.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "Now that the `.py` script has been written, I can put it all together in the next post."
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
