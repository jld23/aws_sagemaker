{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# SM00: SageMaker and Production Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a lot of considerations in moving from a local model used to train and predict on batch data to a production model. This series of posts explores how to create an MLOps compliant production pipeline using AWS's SageMaker Studio.\n",
    "\n",
    "SageMaker Studio is a suite of tools that helps manage the infrastructure and collaboration for a machine learning project in the AWS ecosystem. Some of the biggest advantages of SageMaker Studio include:\n",
    "\n",
    "- Ability to spin up hardware resources as needed\n",
    "- Automatically spin down hardware resources once the task is complete\n",
    "- Ability to create a pipeline to automate the machine learning process from preprocessing data through deploying the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "For brevity, I'll assume that SageMaker Studio and an IAM role with the appropriate permissions have been set up. In a corporate/enterprise environment, these will generally be set up by an administrator or someone on the architecture team.\n",
    "\n",
    "- For directions on setting up the SageMaker environment see [Onboard to Amazon SageMaker Domain Using Quick setup](https://docs.aws.amazon.com/sagemaker/latest/dg/onboard-quick-start.html)\n",
    "- For directions on setting up an AWS account and IAM role see [Set Up Amazon SageMaker Prerequisites](https://docs.aws.amazon.com/sagemaker/latest/dg/gs-set-up.html)\n",
    "\n",
    "The notebooks in this series *may* run on a stand alone SageMaker Jupyter Notebook instance or *possibly* in a local environment where the AWS credentials are specified. However, this series is designed to take advantage of the managed infrastructure and other benefits of using SageMaker Studio, so that will be the prefered environment for all posts in the series. I won't be testing, trying, or troubleshooting the code to work on stand alone SageMaker Jupyter Notebook instances or local environments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Series Guide\n",
    "\n",
    "1. [Read from and Write to S3]()\n",
    "1. [Clean Data]()\n",
    "1. [ETL Pipe Foundations]()\n",
    "1. [ETL (extract, transform, load) Script]()\n",
    "1. [ETL Pipeline]()\n",
    "1. [EDA (Exploratory Data Analysis)]()\n",
    "1. [Develop Preprocessing Code]()\n",
    "1. [Preprocessing Pipeline]()\n",
    "1. [Train Pre-built Model]()\n",
    "1. [Train Custom Model]()\n",
    "1. [Inference]()\n",
    "1. [Multistep Pipeline]()\n",
    "1. [Custom Transformers]()\n",
    "1. [Custom Transformers at Inference]()\n",
    "1. [Hyperparameter Optimization]()\n",
    "1. [Evaluate Model]()\n",
    "1. [Register and Deploy]()\n",
    "1. [Debugger]()\n",
    "1. [Interpretability and Bias]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Practices for Flexibility/Automation\n",
    "\n",
    "Based on experience, we want to keep as much related code in the same place as possible. In the past, our code has spanned different applications, EC2s, repos, and just about everything else you can think of. This made it extremely difficult to track down what code needed to be updated when we needed to make a change.\n",
    "\n",
    "Additionally, we want to consolidate where changes need to be made in the code. In the past, we had hard coded values into several steps of the code. In the current code, the goal is to put hard coded values (in this case, the column names) all in the same script. Should we need to make changes to the included columns, we only have to change the `preprocessing.py` script. *Note*, in our production workflow, the data capture pulls all data in the specified tables regardless of whether that column is expected in the `preprocessing.py` script or not.\n",
    "\n",
    "To update the workflow:\n",
    "\n",
    "- If a new table is available\n",
    "    - Add the table to the features DAG\n",
    "    - Add the columns to the `preprocessing.py` script\n",
    "- If a column was added or removed:\n",
    "    - Add or remove the column(s) in the `preprocessing.py` script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Scripts\n",
    "\n",
    "All Python scripts to be run on EC2s in a pipeline are in the `write_scripts.ipynb`. This means that any changes needed to the Python scripts can be done all at once in the same `ipynb` notebook. Simply run the notebook to update the `.py` scripts.\n",
    "\n",
    "To write these scripts to their own file use the built-in Jupyter magic command `%%writefile filename.py`. Everything in the cell with that magic command will be written to a separate file. Different file types can be written by changing the file extension. Example `filename.py` vs `filename.txt`."
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
