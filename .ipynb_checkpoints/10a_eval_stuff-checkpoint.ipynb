{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SM10: Evaluate Model Script - addtl metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To Do\n",
    "\n",
    "- Fix charts\n",
    "- Fix the 3 broken metrics, probably need to be charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting evaluate_extd.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile evaluate_extd.py\n",
    "import json\n",
    "import logging\n",
    "import pathlib\n",
    "import pickle\n",
    "import tarfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost\n",
    "\n",
    "from sklearn.metrics import(\n",
    "    accuracy_score,\n",
    "#     precision_score,\n",
    "#     recall_score,\n",
    "    confusion_matrix,\n",
    "    roc_curve,\n",
    "#     mine\n",
    "    auc, \n",
    "    precision_recall_curve, \n",
    "    precision_score,\n",
    "    average_precision_score,\n",
    "    roc_auc_score,\n",
    "    log_loss,\n",
    "    f1_score,\n",
    "    recall_score,\n",
    "    roc_curve,\n",
    "    make_scorer,\n",
    "    balanced_accuracy_score,\n",
    "    cohen_kappa_score,\n",
    "    matthews_corrcoef,\n",
    "    fbeta_score)\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.addHandler(logging.StreamHandler())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model_path = \"/opt/ml/processing/model/model.tar.gz\"\n",
    "    with tarfile.open(model_path) as tar:\n",
    "        tar.extractall(path=\"..\")\n",
    "        \n",
    "    logger.debug(\"Loading xgboost model\")\n",
    "    model = pickle.load(open(\"xgboost-model\", \"rb\"))\n",
    "    \n",
    "    logger.debug(\"Loading test input data\")\n",
    "    test_path = \"/opt/ml/processing/test/test_feats.csv\"\n",
    "    df = pd.read_csv(test_path)\n",
    "    \n",
    "    logger.debug(\"Reading test data\")\n",
    "    y_test = df.iloc[:, 0].to_numpy()\n",
    "    df.drop(df.columns[0], axis=1, inplace=True)\n",
    "    X_test = xgboost.DMatrix(df.values)\n",
    "    \n",
    "    logger.info(\"Performing predictions against test data\")\n",
    "    prediction_probabilities = model.predict(X_test)\n",
    "    predictions = np.round(prediction_probabilities)\n",
    "    \n",
    "#     precision = precision_score(y_test, predictions)\n",
    "#     recall = recall_score(y_test, predictions)\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    conf_matrix = confusion_matrix(y_test, predictions)\n",
    "    fpr, tpr, _ = roc_curve(y_test, prediction_probabilities)\n",
    "    \n",
    "    auc_score = auc(fpr, tpr) \n",
    "    pr_curve = precision_recall_curve(y_test, predictions)\n",
    "    precision = precision_score(y_test, predictions)\n",
    "    avg_precision = average_precision_score(y_test, predictions)\n",
    "    roc_auc = roc_auc_score(y_test, predictions)\n",
    "#     log_loss_score = log_loss(y_test, predictions)\n",
    "    f1 = f1_score(y_test, predictions)\n",
    "    recall = recall_score(y_test, predictions)\n",
    "    roc = roc_curve(y_test, predictions)\n",
    "    informedness = balanced_accuracy_score(y_test, predictions, adjusted=True)\n",
    "    cohen_kappa = cohen_kappa_score(y_test, predictions)\n",
    "    matthews_coef = matthews_corrcoef(y_test, predictions)\n",
    "    fbeta = fbeta_score(y_test, predictions, beta=0.5)\n",
    "        \n",
    "    logger.debug(\"Accuracy: {}\".format(accuracy))\n",
    "#     logger.debug(\"Precision: {}\".format(precision))\n",
    "#     logger.debug(\"Recall: {}\".format(recall))\n",
    "    logger.debug(\"Confusion matrix: {}\".format(conf_matrix))\n",
    "    \n",
    "    logger.debug(\"AUC: {}\".format(auc_score))\n",
    "    logger.debug(\"Precision Recall Curve: {}\".format(pr_curve))\n",
    "    logger.debug(\"Precision: {}\".format(precision))\n",
    "    logger.debug(\"Average Percision: {}\".format(avg_precision))\n",
    "    logger.debug(\"ROC AUC: {}\".format(roc_auc))\n",
    "#     logger.debug(\"Log loss: {}\".format(log_loss_score))\n",
    "    logger.debug(\"F1: {}\".format(f1))\n",
    "    logger.debug(\"Recall: {}\".format(recall))\n",
    "    logger.debug(\"ROC: {}\".format(roc))\n",
    "    logger.debug(\"informedness: {}\".format(informedness))\n",
    "    logger.debug(\"Cohen Kappa: {}\".format(cohen_kappa))\n",
    "    logger.debug(\"Mathews Correlation Coefficient: {}\".format(matthews_coef))\n",
    "    logger.debug(\"Fbeta: {}\".format(fbeta))\n",
    "    \n",
    "    report_dict = {\n",
    "        \"binary_classification_metrics\": {\n",
    "            \"auc\": {\"value\":auc_score, \"standard_deviation\":\"NaN\"},\n",
    "            \"precision\": {\"value\":precision, \"standard_deviation\":\"NaN\"},\n",
    "            \"avg_percision\": {\"value\":avg_precision, \"standard_deviation\":\"NaN\"},\n",
    "            \"roc_auc\": {\"value\":roc_auc, \"standard_deviation\":\"NaN\"},\n",
    "#             \"log_loss\": {\"value\":log_loss_score, \"standard_deviation\":\"NaN\"},\n",
    "            \"f1\": {\"value\":f1, \"standard_deviation\":\"NaN\"},\n",
    "            \"recall\": {\"value\":recall, \"standard_deviation\":\"NaN\"},\n",
    "#             \"roc\": {\"value\":roc, \"standard_deviation\":\"NaN\"},\n",
    "            \"informedness\": {\"value\":informedness, \"standard_deviation\":\"NaN\"},\n",
    "            \"cohen_kappa\": {\"value\":cohen_kappa, \"standard_deviation\":\"NaN\"},\n",
    "            \"mathews_coef\": {\"value\":matthews_coef, \"standard_deviation\":\"NaN\"},\n",
    "            \"fbeta\": {\"value\":fbeta, \"standard_deviation\":\"NaN\"},\n",
    "            \"accuracy\": {\"value\":accuracy, \"standard_deviation\":\"NaN\"},\n",
    "#             \"pr_curve\": {\"0\": {\"0\": int(pr_curve[0][0]), \"1\": int(pr_curve[0][1])},\n",
    "#                          \"1\": {\"0\": int(pr_curve[1][0]), \"1\": int(pr_curve[1][1])},\n",
    "            \"confusion_matrix\": {\"0\": {\"0\": int(conf_matrix[0][0]), \"1\": int(conf_matrix[0][1])},\n",
    "                                 \"1\": {\"0\": int(conf_matrix[1][0]), \"1\": int(conf_matrix[1][1])}\n",
    "                                },\n",
    "            \"receiver_operating_charastic_curve\": {\n",
    "                \"false_positive_rates\": list(fpr),\n",
    "                \"true_positive_rates\": list(tpr)\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print(report_dict)\n",
    "    output_dir = \"/opt/ml/processing/evaluation\"\n",
    "    pathlib.Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    evaluation_path = f'{output_dir}/evaluation.json'\n",
    "    with open(evaluation_path, \"w\") as f:\n",
    "        f.write(json.dumps(report_dict))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No finished training job found associated with this estimator. Please make sure this estimator is only used for building workflow config\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "_PipelineExecution(arn='arn:aws:sagemaker:us-east-1:707031497630:pipeline/insexample/execution/tp906pdgp244', sagemaker_session=<sagemaker.session.Session object at 0x7f0a45e8fac0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sagemaker\n",
    "import sagemaker.session\n",
    "\n",
    "from sagemaker.workflow.parameters import (\n",
    "    ParameterInteger,\n",
    "    ParameterString,\n",
    ")\n",
    "\n",
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.workflow.steps import ProcessingStep\n",
    "\n",
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "from sagemaker.workflow.functions import Join\n",
    "from sagemaker.workflow.execution_variables import ExecutionVariables\n",
    "\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.workflow.steps import TrainingStep\n",
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "\n",
    "from sagemaker.processing import ScriptProcessor\n",
    "from sagemaker.workflow.properties import PropertyFile\n",
    "\n",
    "from sagemaker.model_metrics import MetricsSource, ModelMetrics\n",
    "from sagemaker.workflow.step_collections import RegisterModel\n",
    "\n",
    "session = sagemaker.session.Session()\n",
    "region = session.boto_region_name\n",
    "role = sagemaker.get_execution_role()\n",
    "bucket = session.default_bucket()\n",
    "prefix = '1_ins_dataset'\n",
    "\n",
    "pipeline_name = \"InsExample\"  # SageMaker Pipeline name\n",
    "model_package_group_name = \"Insurance-Co-Example\"  # Model name in model registry\n",
    "framework_version = \"0.23-1\"\n",
    "\n",
    "train_uri = f's3://{bucket}/{prefix}/final/train/train_feats.csv'\n",
    "validate_uri = f's3://{bucket}/{prefix}/final/validate/validate_feats.csv'\n",
    "test_uri = f's3://{bucket}/{prefix}/final/test/test_feats.csv'\n",
    "\n",
    "\n",
    "# tags = [\n",
    "#     {\"Key\": \"DATASET\", \"Value\": \"InsCOIL\"},\n",
    "#     {\"Key\": \"SOURCE\", \"Value\": \"UCI\"}\n",
    "#    ]\n",
    "\n",
    "processing_instance_count = ParameterInteger(name=\"ProcessingInstanceCount\", default_value=1)\n",
    "processing_instance_type = ParameterString(\n",
    "    name=\"ProcessingInstanceType\", default_value=\"ml.t3.medium\")\n",
    "\n",
    "training_instance_type = ParameterString(\n",
    "    name=\"TrainingInstanceType\", default_value=\"ml.m5.xlarge\")\n",
    "    \n",
    "train_data = ParameterString(\n",
    "    name=\"TrainData\",\n",
    "    default_value=train_uri\n",
    ")\n",
    "validate_data = ParameterString(\n",
    "    name=\"ValidateData\",\n",
    "    default_value=validate_uri\n",
    ")\n",
    "test_data = ParameterString(\n",
    "    name=\"TestData\",\n",
    "    default_value=test_uri\n",
    ")\n",
    "\n",
    "model_approval_status = ParameterString(\n",
    "    name='ModelApprovalStatus',\n",
    "    default_value='PendingManualApproval'\n",
    ")\n",
    "\n",
    "image_uri = sagemaker.image_uris.retrieve(\n",
    "    framework='xgboost',\n",
    "    region=region,\n",
    "    version='1.2-2',\n",
    "    py_version='py3',\n",
    "    instance_type='ml.m5.xlarge')\n",
    "\n",
    "xgb_estimator = Estimator(\n",
    "    image_uri=image_uri,\n",
    "    instance_type=training_instance_type,\n",
    "    instance_count=1,\n",
    "    role=role,\n",
    "    disable_profiler=True,\n",
    "    output_path=Join(\n",
    "        on=\"/\",\n",
    "        values=[\n",
    "            \"s3://{}\".format(bucket),\n",
    "            prefix,\n",
    "            ExecutionVariables.PIPELINE_EXECUTION_ID,\n",
    "            \"model\"],\n",
    "            ))\n",
    "\n",
    "xgb_estimator.set_hyperparameters(\n",
    "    max_depth=5,\n",
    "    eta=0.2,\n",
    "    gamma=4,\n",
    "    min_child_weight=6,\n",
    "    subsample=0.8,\n",
    "    objective='binary:logistic',\n",
    "    num_round=25)\n",
    "\n",
    "step_train = TrainingStep(\n",
    "    name='train_model',\n",
    "    estimator=xgb_estimator,\n",
    "    inputs={\n",
    "        'train':TrainingInput(\n",
    "            s3_data=train_data,\n",
    "            content_type='text/csv'),\n",
    "        'validation':TrainingInput(\n",
    "            s3_data=validate_data,\n",
    "            content_type='text/csv')\n",
    "            })\n",
    "\n",
    "evaluate_model_processor = ScriptProcessor(\n",
    "    image_uri=image_uri,\n",
    "    command=[\"python3\"],\n",
    "    instance_type=processing_instance_type,\n",
    "    instance_count=processing_instance_count,\n",
    "    role=role,\n",
    "    base_job_name=\"ins-example-job\")\n",
    "\n",
    "evaluation_report = PropertyFile(\n",
    "    name='EvaluationReport',\n",
    "    output_name='evaluation',\n",
    "    path='evaluation.json')\n",
    "\n",
    "step_evaluate = ProcessingStep(\n",
    "    name='evaluate_model',\n",
    "    processor=evaluate_model_processor,\n",
    "    inputs=[\n",
    "        ProcessingInput(\n",
    "            source=step_train.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "            destination=\"/opt/ml/processing/model\"\n",
    "        ),\n",
    "        ProcessingInput(\n",
    "            source=test_data,\n",
    "            destination=\"/opt/ml/processing/test\"\n",
    "        )\n",
    "    ],\n",
    "    outputs = [\n",
    "        ProcessingOutput(\n",
    "            output_name='evaluation',\n",
    "            source='/opt/ml/processing/evaluation',\n",
    "            destination=Join(\n",
    "                on='/',\n",
    "                values=[\n",
    "                    's3://{}'.format(bucket),\n",
    "                    prefix,\n",
    "                    ExecutionVariables.PIPELINE_EXECUTION_ID,\n",
    "                    'evaluation-report']\n",
    "            )\n",
    "        )\n",
    "    ],\n",
    "    code='evaluate_extd.py',\n",
    "    property_files=[evaluation_report]\n",
    ")\n",
    "\n",
    "model_metrics = ModelMetrics(\n",
    "    model_statistics=MetricsSource(\n",
    "        s3_uri=Join(\n",
    "            on='/',\n",
    "            values=[\n",
    "                step_evaluate.arguments[\"ProcessingOutputConfig\"][\"Outputs\"][0]['S3Output']['S3Uri'],\n",
    "                'evaluation.json']\n",
    "        ),\n",
    "        content_type='application/json')\n",
    ")\n",
    "\n",
    "step_register = RegisterModel(\n",
    "    name='register-model',\n",
    "    estimator=xgb_estimator,\n",
    "    model_data=step_train.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "    content_types=['text/csv'],\n",
    "    response_types=['text/csv'],\n",
    "    inference_instances=['ml.t2.medium', 'ml.m5.xlarge', 'ml.m5.large'],\n",
    "    transform_instances=['ml.m5.xlarge'],\n",
    "    model_package_group_name=model_package_group_name,\n",
    "    approval_status=model_approval_status,\n",
    "    model_metrics=model_metrics)\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    name=pipeline_name,\n",
    "    parameters=[\n",
    "        processing_instance_type,\n",
    "        processing_instance_count,\n",
    "        training_instance_type,\n",
    "        train_data,\n",
    "        validate_data,\n",
    "        test_data,\n",
    "        model_approval_status\n",
    "    ],\n",
    "    steps=[step_train, step_evaluate, step_register])\n",
    "\n",
    "pipeline.upsert(role_arn=role, tags=tags)\n",
    "\n",
    "pipeline.start(execution_display_name=\"InsPrebuiltModelEvalExtd-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import boto3\n",
    "import logging\n",
    "import os\n",
    "import warnings\n",
    "import joblib\n",
    "import argparse\n",
    "\n",
    "warnings.simplefilter(\"once\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_validate\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# from hyperopt import hp\n",
    "from sklearn.metrics import auc, precision_recall_curve, precision_score, average_precision_score\n",
    "from sklearn.metrics import roc_auc_score, log_loss, f1_score, recall_score, roc_curve\n",
    "from sklearn.metrics import make_scorer, balanced_accuracy_score, cohen_kappa_score, matthews_corrcoef, fbeta_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(feats_filepath, gt_filepath):\n",
    "    feats = pd.read_parquet(feats_filepath).set_index(index_col)\n",
    "    target = pd.read_parquet(gt_filepath).set_index(index_col).fillna(0)\n",
    "    full_df = feats.merge(target, on=index_col, how='inner')\n",
    "    feats = full_df.drop(columns=list(target.columns)).reset_index()\n",
    "    target = full_df[list(target.columns)].reset_index()\n",
    "    return feats, target\n",
    "\n",
    "index_col = 'transaction_id'\n",
    "\n",
    "informedness = make_scorer(balanced_accuracy_score, adjusted=True)\n",
    "kappa = make_scorer(cohen_kappa_score)\n",
    "mcc = make_scorer(matthews_corrcoef)\n",
    "fbet = make_scorer(fbeta_score, beta=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "session = sagemaker.session.Session()\n",
    "region = session.boto_region_name\n",
    "role = sagemaker.get_execution_role()\n",
    "bucket = 'asurion-fraud-data-science-prod'\n",
    "prefix = \"split_data/2022-10-31\"\n",
    "event_param = 'fulfill'\n",
    "index_col = 'transaction_id'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_filepath = f's3://{bucket}/{prefix}/fulfill/final/train/train_feats.parquet'\n",
    "gt_filepath = f's3://{bucket}/{prefix}/fulfill/processed/ground_truth/org_fraud_gt.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training XGBoost model')\n",
    "estimator = XGBClassifier()\n",
    "estimator.fit(train_feats.dropna(axis=1, how='all').set_index(index_col), train_target['org_fraud'])\n",
    "print('Finished training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Cross-Validating model with {} samples'.format(train_feats.shape[0]))\n",
    "report1_dict = cross_validate(estimator, train_feats.dropna(how='all', axis=1).set_index(index_col),\n",
    "                              train_target['org_fraud'], cv=5, return_estimator=True, error_score=\"raise\",\n",
    "                              scoring=('roc_auc', 'average_precision', 'precision', 'recall', 'f1', 'neg_log_loss', 'neg_brier_score'))\n",
    "print(\"roc_auc and average_percision are the important ones here\")\n",
    "print(report1_dict)\n",
    "\n",
    "report2_dict = cross_validate(estimator, train_feats.dropna(how='all', axis=1).set_index(index_col),\n",
    "                              train_target['org_fraud'], cv=5, return_estimator=True, error_score=\"raise\",\n",
    "                              scoring={'informedness':informedness, 'cohen_kappa':kappa, 'matthews_corr':mcc, 'f_beta':fbet})\n",
    "print(\"Additional metrics\")\n",
    "print(report2_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "#     parser = argparse.ArgumentParser()\n",
    "    \n",
    "#     parser.add_argument('--output-data-dir', type=str, default=os.environ.get('SM_OUTPUT_DATA_DIR'))\n",
    "#     parser.add_argument('--model-dir', type=str, default=os.environ.get('SM_MODEL_DIR'))\n",
    "#     parser.add_argument('--feats-dir', type=str, default=os.environ.get('SM_CHANNEL_TRAIN_DATA'))\n",
    "#     parser.add_argument('--labels-dir', type=str, default=os.environ.get('SM_CHANNEL_TRAIN_LABELS'))\n",
    "    \n",
    "#     args = parser.parse_args()\n",
    "    \n",
    "#     print(\"Args: {}\".format(args))\n",
    "    \n",
    "    print('Saving model')\n",
    "    joblib.dump(estimator, os.path.join(args.model_dir, 'model.joblib'))\n",
    "    \n",
    "def model_fn(model_dir):\n",
    "    \"\"\"Deserialize fitted model\n",
    "    \"\"\"\n",
    "    model = joblib.load(os.path.join(model_dir, \"model.joblib\"))\n",
    "    return model\n",
    "    \n",
    "def input_fn(input_data, content_type):\n",
    "    \"\"\"Parse input data payload\n",
    "\n",
    "    We currently take csv, parquet, and json input. We only process data without the target value.\n",
    "    \"\"\"\n",
    "    if content_type == 'text/csv':\n",
    "        # Read the raw input data as CSV.\n",
    "        df = pd.read_csv(input_data)\n",
    "    elif content_type = 'application/x-parquet':\n",
    "        df = pd.read_parquet(input_data)\n",
    "    elif content_type = 'application/json':\n",
    "        df = pd.read_json(input_data)\n",
    "    else:\n",
    "        raise ValueError(\"{} not supported by script!\".format(content_type))\n",
    "    return df\n",
    "\n",
    "def predict_fn(input_data, model):\n",
    "    \"\"\"Preprocess input data\n",
    "\n",
    "    We implement this because the default predict_fn uses .predict(), but our model is a preprocessor\n",
    "    so we want to use .transform().\n",
    "\n",
    "    The output is returned in the following order:\n",
    "\n",
    "        rest of features either one hot encoded or standardized\n",
    "    \"\"\"\n",
    "    \n",
    "    print('Making predictions')\n",
    "    preds = model.predict(input_data)\n",
    "    return preds\n",
    "\n",
    "def output_fn(prediction, response_content_type):\n",
    "    \"\"\"Format prediction output\n",
    "\n",
    "    The default accept/content-type between containers for serial inference is JSON.\n",
    "    We also want to set the ContentType or mimetype as the same value as accept so the next\n",
    "    container can read the response payload correctly.\n",
    "    \"\"\"\n",
    "    if response_content_type == \"application/json\":\n",
    "        return worker.Response(json.dumps(prediction), mimetype=response_content_type)\n",
    "    elif accept == 'text/csv':\n",
    "        df_response = pd.DataFrame(prediction)\n",
    "        csv_response = df_response.to_csv(index=False)\n",
    "        return worker.Response(csv_response, mimetype=response_content_type)\n",
    "    else:\n",
    "        raise RuntimeException(\"{} accept type is not supported by this script.\".format(response_content_type))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Data Science 2.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
