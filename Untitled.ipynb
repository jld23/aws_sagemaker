{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting preprocess.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile preprocess.py\n",
    "# import subprocess\n",
    "# import sys\n",
    "\n",
    "# def install(package):\n",
    "#     subprocess.check_call([sys.executable, \"-q\", \"-m\", \"pip\", \"install\", package])\n",
    "# def upgrade(package):\n",
    "#     subprocess.check_call([sys.executable, \"-q\", \"-m\", \"pip\", \"install\", package, '--upgrade'])\n",
    "    \n",
    "# upgrade('pandas==1.3.5')\n",
    "# upgrade('numpy')\n",
    "# upgrade('pyarrow')\n",
    "# install('category_encoders')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import warnings\n",
    "import joblib\n",
    "import argparse\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# import category_encoders as ce\n",
    "\n",
    "cat_cols = ['zip_agg Customer Subtype', 'zip_agg Customer main type']\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    input_path = '/opt/ml/processing/input'\n",
    "    output_path = '/opt/ml/processing/output'\n",
    "    \n",
    "    try:\n",
    "        os.makedirs(os.path.join(output_path, \"train\"))\n",
    "        os.makedirs(os.path.join(output_path, \"validate\"))\n",
    "        os.makedirs(os.path.join(output_path, \"test\"))\n",
    "        os.makedirs(os.path.join(output_path, 'encoder'))\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    print(\"Reading data\")\n",
    "    df = pd.read_csv(os.path.join(input_path, 'full_data.csv'))\n",
    "    \n",
    "    print('splitting data')\n",
    "    train_data, validation_data, test_data = np.split(\n",
    "        df.sample(frac=1, random_state=1420),\n",
    "        [int(0.7 * len(df)), int(0.9 * len(df))],)\n",
    "    \n",
    "    print('Preprocessing data')\n",
    "    sk_encoder = OneHotEncoder()\n",
    "    sk_one_hot = sk_encoder.fit_transform(train_data[cat_cols]).toarray()\n",
    "    sk_one_hot = pd.DataFrame(sk_one_hot, columns=sk_encoder.get_feature_names())\n",
    "    sk_one_hot.columns = [x.replace('x0', cat_cols[0]).replace('x1', cat_cols[1]) for x in sk_one_hot.columns]\n",
    "    train_data = sk_one_hot.merge(train_data.drop(columns=cat_cols), left_index=True, right_index=True)\n",
    "    \n",
    "    sk_test = sk_encoder.transform(test_data[cat_cols]).toarray()\n",
    "    sk_test = pd.DataFrame(sk_test, columns=sk_encoder.get_feature_names())\n",
    "    sk_test.columns = [x.replace('x0', cat_cols[0]).replace('x1', cat_cols[1]) for x in sk_test.columns]\n",
    "    test_data = sk_test.merge(test_data.drop(columns=cat_cols), left_index=True, right_index=True)\n",
    "    \n",
    "    # ce_encoder = ce.OneHotEncoder(cols=cat_cols, use_cat_names=True, handle_missing='return_nan')\n",
    "    # trnsfrmd_df = ce_encoder.fit_transform(df)\n",
    "    \n",
    "    print('Saving dataframe')\n",
    "    train_data.to_parquet(os.path.join(output_path, 'train', 'train_feats.parquet'))\n",
    "    test_data.to_parquet(os.path.join(output_path, 'test', 'test_feats.parquet'))\n",
    "    validation_data.to_parquet(os.path.join(output_path, 'validate', 'validate_feats.parquet'))\n",
    "                              \n",
    "    print('Saving preprocessor joblib')\n",
    "    encoder_name = 'preprocessor.joblib'\n",
    "    joblib.dump(sk_encoder, os.path.join(output_path, 'encoder', encoder_name))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The input argument instance_type of function (sagemaker.image_uris.retrieve) is a pipeline variable (<class 'sagemaker.workflow.parameters.ParameterString'>), which is not allowed. The default_value of this Parameter object will be used to override it.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "_PipelineExecution(arn='arn:aws:sagemaker:us-east-1:707031497630:pipeline/insexample/execution/1donc66rpim8', sagemaker_session=<sagemaker.session.Session object at 0x7f051da70b10>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sagemaker\n",
    "import sagemaker.session\n",
    "\n",
    "from sagemaker.workflow.parameters import (\n",
    "    ParameterInteger,\n",
    "    ParameterString)\n",
    "\n",
    "from sagemaker.workflow.functions import Join\n",
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "from sagemaker.workflow.steps import ProcessingStep\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "\n",
    "session = sagemaker.session.Session()\n",
    "region = session.boto_region_name\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "bucket = session.default_bucket()\n",
    "prefix = '1_ins_dataset'\n",
    "pipeline_name = \"InsExample\"  # SageMaker Pipeline name\n",
    "model_package_group_name = \"Insurance Co Example\"  # Model name in model registry\n",
    "framework_version = \"0.23-1\"\n",
    "\n",
    "processing_instance_count = ParameterInteger(name=\"ProcessingInstanceCount\", default_value=1)\n",
    "\n",
    "processing_instance_type = ParameterString(\n",
    "    name=\"ProcessingInstanceType\", default_value=\"ml.t3.medium\")\n",
    "\n",
    "input_uri = Join(on=\"/\", values=['s3://{}'.format(bucket),\n",
    "                                 prefix,\n",
    "                                 'clean',\n",
    "                                 'full_data.csv'])\n",
    "\n",
    "tags = [\n",
    "    {\"Key\": \"DATASET\", \"Value\": \"InsCOIL\"},\n",
    "    {\"Key\": \"SOURCE\", \"Value\": \"UCI\"}\n",
    "   ]\n",
    "\n",
    "sklearn_processor = SKLearnProcessor(\n",
    "    framework_version=framework_version,\n",
    "    role=role,\n",
    "    instance_type=processing_instance_type,\n",
    "    instance_count=processing_instance_count,\n",
    "    base_job_name=\"ins-example-job\"\n",
    ")\n",
    "\n",
    "preprocess_step = ProcessingStep(\n",
    "    name=\"preprocess\",\n",
    "    processor=sklearn_processor,\n",
    "    inputs=[\n",
    "        ProcessingInput(source=input_uri, destination=\"/opt/ml/processing/input\")\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(\n",
    "            output_name=\"train\",\n",
    "            source=\"/opt/ml/processing/output/train\",\n",
    "            destination=Join(\n",
    "                on=\"/\",\n",
    "                values=[\n",
    "                    \"s3://{}\".format(bucket),\n",
    "                    prefix,\n",
    "                    'final',\n",
    "                    'train'\n",
    "                ]\n",
    "            )\n",
    "        ),\n",
    "        ProcessingOutput(\n",
    "            output_name=\"test\",\n",
    "            source=\"/opt/ml/processing/output/test\",\n",
    "            destination=Join(\n",
    "                on=\"/\",\n",
    "                values=[\n",
    "                    \"s3://{}\".format(bucket),\n",
    "                    prefix,\n",
    "                    'final',\n",
    "                    'test'\n",
    "                ]\n",
    "            )\n",
    "        ),\n",
    "        ProcessingOutput(\n",
    "            output_name=\"validate\",\n",
    "            source=\"/opt/ml/processing/output/validate\",\n",
    "            destination=Join(\n",
    "                on=\"/\",\n",
    "                values=[\n",
    "                    \"s3://{}\".format(bucket),\n",
    "                    prefix,\n",
    "                    'final',\n",
    "                    'validate'\n",
    "                ]\n",
    "            )\n",
    "        ),\n",
    "        ProcessingOutput(\n",
    "            output_name=\"encoder\",\n",
    "            source=\"/opt/ml/processing/output/encoder\",\n",
    "            destination=Join(\n",
    "                on=\"/\",\n",
    "                values=[\n",
    "                    \"s3://{}\".format(bucket),\n",
    "                    prefix,\n",
    "                    'final',\n",
    "                    'encoder'\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "    ],\n",
    "    code=\"preprocess.py\"\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    name=pipeline_name,\n",
    "    parameters=[\n",
    "        processing_instance_type,\n",
    "        processing_instance_count\n",
    "    ],\n",
    "    steps=[preprocess_step])\n",
    "\n",
    "pipeline.upsert(role_arn=role, tags=tags)\n",
    "\n",
    "pipeline.start(\n",
    "    execution_display_name=\"InsClean-preprocess2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
