{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be9f1393-83b2-4bf5-9582-4a6a7f4eaa00",
   "metadata": {},
   "source": [
    "# Code to Replicate Preprocessor as an Estimator Failure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d80f144-8ec0-49a8-9764-51d29ad83d78",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import random\n",
    "\n",
    "import sagemaker\n",
    "import sagemaker.session\n",
    "\n",
    "from sagemaker.workflow.parameters import (\n",
    "    ParameterInteger,\n",
    "    ParameterString\n",
    ")\n",
    "\n",
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.workflow.steps import ProcessingStep, TrainingStep\n",
    "from sagemaker.workflow.functions import Join\n",
    "from sagemaker.workflow.execution_variables import ExecutionVariables\n",
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "\n",
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from time import gmtime, strftime, sleep\n",
    "import boto3\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be76b3d6-8d27-41b9-9656-1a15edcd77a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "session = sagemaker.session.Session()\n",
    "region = session.boto_region_name\n",
    "role = sagemaker.get_execution_role()\n",
    "bucket = session.default_bucket()\n",
    "prefix = 'custom_preprocessing'\n",
    "\n",
    "timestamp_suffix = strftime(\"%Y-%m-%d-%H%M%S\", gmtime())\n",
    "folder_name = prefix + '-' + timestamp_suffix\n",
    "prefix_path = f's3://{bucket}/{folder_name}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ffc3e6e-42a7-473d-aac6-2275a4225cdd",
   "metadata": {},
   "source": [
    "## Create sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b612b10-10c5-4157-9a7b-e1256a3b56a5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>true_false</th>\n",
       "      <th>one_hot</th>\n",
       "      <th>dates</th>\n",
       "      <th>floats</th>\n",
       "      <th>max_of_list</th>\n",
       "      <th>nunique_of_list</th>\n",
       "      <th>desc_stats</th>\n",
       "      <th>multi_label</th>\n",
       "      <th>random_col</th>\n",
       "      <th>other</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>true</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022-07-10</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3,0,9,4,2</td>\n",
       "      <td>apple,orange,grape</td>\n",
       "      <td>9,2,8,3,4</td>\n",
       "      <td>apple,orange,grape</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>false</td>\n",
       "      <td>purple</td>\n",
       "      <td>2022-05-07</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0,9,8,3,4,3,3,4,9</td>\n",
       "      <td>1</td>\n",
       "      <td>pineapple,grape,strawberry</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>orange</td>\n",
       "      <td>2022-10-01</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0,2,3,9,8,4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7,8,9,2,3,4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>purple</td>\n",
       "      <td>2022-10-12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>4,4,4,4,4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>blueberry</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>blue</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5,4,3</td>\n",
       "      <td>pineapple</td>\n",
       "      <td>34</td>\n",
       "      <td>grapefruit,apple</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  true_false one_hot       dates  floats  max_of_list     nunique_of_list  \\\n",
       "0       true     NaN  2022-07-10     3.0    3,0,9,4,2  apple,orange,grape   \n",
       "1      false  purple  2022-05-07     8.0          NaN   0,9,8,3,4,3,3,4,9   \n",
       "2        NaN  orange  2022-10-01     2.0  0,2,3,9,8,4                 NaN   \n",
       "3          1  purple  2022-10-12     NaN            4           4,4,4,4,4   \n",
       "4          0    blue         NaN     4.0        5,4,3           pineapple   \n",
       "\n",
       "    desc_stats                 multi_label  random_col  other  \n",
       "0    9,2,8,3,4          apple,orange,grape         NaN      0  \n",
       "1            1  pineapple,grape,strawberry         3.0      1  \n",
       "2  7,8,9,2,3,4                         NaN         6.0      2  \n",
       "3          NaN                   blueberry         1.0      3  \n",
       "4           34            grapefruit,apple         NaN      4  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_vals = ['true', 'false', np.nan, '1', '0']\n",
    "onehot_vals = [np.nan, 'purple', 'orange', 'purple', 'blue']\n",
    "\n",
    "date_vals = []\n",
    "for _ in range(4):\n",
    "    date = datetime.date(2022, random.randint(1, 12), random.randint(1, 31))\n",
    "    date_vals.append(date)\n",
    "date_vals.append(np.nan)\n",
    "\n",
    "float_vals = [3, 8.0, 2, np.nan, 4.0]\n",
    "list_max_vals = ['3,0,9,4,2', np.nan, '0,2,3,9,8,4', '4', '5,4,3']\n",
    "list_nunique_vals = ['apple,orange,grape', '0,9,8,3,4,3,3,4,9', np.nan, '4,4,4,4,4', 'pineapple']\n",
    "descstat_vals = ['9,2,8,3,4', '1', '7,8,9,2,3,4', np.nan, '34']\n",
    "multi_label_vals = ['apple,orange,grape', 'pineapple,grape,strawberry', np.nan, 'blueberry', 'grapefruit,apple']\n",
    "drop_vals = [np.nan, 3, 6, 1, np.nan]\n",
    "x_rand = list(range(5))\n",
    "\n",
    "sample_df = pd.DataFrame({\n",
    "    'true_false':tf_vals,\n",
    "    'one_hot':onehot_vals,\n",
    "    'dates':date_vals,\n",
    "    'floats':float_vals,\n",
    "    'max_of_list':list_max_vals,\n",
    "    'nunique_of_list':list_nunique_vals,\n",
    "    'desc_stats':descstat_vals,\n",
    "    'multi_label':multi_label_vals,\n",
    "    'random_col':drop_vals,\n",
    "    'other':x_rand})\n",
    "sample_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0a672bb-18f8-428d-8eeb-ebd298b72f04",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_df.to_csv('data/sample.csv')\n",
    "train_input = session.upload_data('data/sample.csv', bucket=bucket, key_prefix=folder_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a3f911-a223-4376-bc2d-a83602cc831a",
   "metadata": {},
   "source": [
    "## Write Preprocessing Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "998c8a5f-c270-46bc-ae74-e89efdf9ff2d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing preprocess.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile preprocess.py\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def upgrade(package):\n",
    "    subprocess.check_call([sys.executable, \"-q\", \"-m\", \"pip\", \"install\", package, '--upgrade'])\n",
    "    \n",
    "upgrade('pandas==1.3.5')\n",
    "upgrade('numpy')\n",
    "upgrade('pyarrow')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import boto3\n",
    "# import logging\n",
    "import os\n",
    "import warnings\n",
    "import joblib\n",
    "import argparse\n",
    "\n",
    "env_parser = argparse.ArgumentParser()\n",
    "env_parser.add_argument('--INPUT_FEATURES_SIZE', type=int, dest='INPUT_FEATURES_SIZE')\n",
    "    \n",
    "env_args = env_parser.parse_args()\n",
    "INPUT_FEATURES_SIZE = env_args.INPUT_FEATURES_SIZE\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "warnings.simplefilter(\"once\")\n",
    "\n",
    "true_false = ['true_false']\n",
    "one_hot = ['one_hot']\n",
    "date_cols = ['dates']\n",
    "float_cols = ['floats']\n",
    "max_of_list = ['max_of_list']\n",
    "count_unique = ['nunique_of_list']\n",
    "desc_stat_cols = ['desc_stats']\n",
    "list_to_labels = ['multi_label']\n",
    "drop_cols = ['random_col']\n",
    "\n",
    "class TrueFalseTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self._col_names = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self._col_names = list(X.columns)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        print('Running TrueFalseTransformer')\n",
    "        X.fillna('-1', inplace=True)\n",
    "        X = X.replace({'true':'1', 'false':'0'})\n",
    "        X = X.apply(pd.to_numeric, args=('coerce',))\n",
    "        return X\n",
    "\n",
    "    def get_feature_names(self):\n",
    "        return self._col_names\n",
    "\n",
    "class OneHotTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self._filler = 'ml_empty'\n",
    "        self._col_names = None\n",
    "        self._encoder = None\n",
    "        self._transformer = None\n",
    "        self._transformed_feats = []\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self._col_names = X.dropna(axis=1, how='all').columns\n",
    "        X = X[self._col_names].fillna(self._filler)\n",
    "        self._encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "        self._transformer = self._encoder.fit(X)\n",
    "        self._transformed_feats = self._transformer.get_feature_names_out()\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        print('Running OneHotTransformer')\n",
    "        X = self._transformer.transform(X[self._col_names])\n",
    "        return X\n",
    "\n",
    "    def get_feature_names(self):\n",
    "        return self._transformed_feats\n",
    "\n",
    "class DateTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self._col_names = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        print('Running DateTransformer')\n",
    "        temp_df = pd.DataFrame(index=X.index.copy())\n",
    "\n",
    "        for col in X.columns:\n",
    "            X[col] = pd.to_datetime(X[col])\n",
    "            temp_df[f'{col}-month'] = X[col].dt.month.astype(float)\n",
    "            temp_df[f'{col}-day_of_week'] = X[col].dt.dayofweek.astype(float)\n",
    "            temp_df[f'{col}-hour'] = X[col].dt.hour.astype(float)\n",
    "            temp_df[f'{col}-day_of_month'] = X[col].dt.day.astype(float)\n",
    "            temp_df[f'{col}-is_month_start'] = X[col].dt.is_month_start.astype(int)\n",
    "            temp_df[f'{col}-is_month_end'] = X[col].dt.is_month_end.astype(int)\n",
    "        self._col_names = list(temp_df.columns)\n",
    "        return temp_df\n",
    "\n",
    "    def get_feature_names(self):\n",
    "        return self._col_names\n",
    "\n",
    "class FloatTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self._col_names = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self._col_names = list(X.columns)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        print('Running FloatTransformer')\n",
    "        for col in self._col_names:\n",
    "            if X[col].dtype == 'string':\n",
    "                X[col] = X[col].astype(float)\n",
    "        return X\n",
    "\n",
    "    def get_feature_names(self):\n",
    "        return self._col_names\n",
    "\n",
    "class ListMaxTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self._col_names = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self._col_names = list(X.columns)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        print('Running ListMaxTransformer')\n",
    "        temp_df = pd.DataFrame(index=X.index)\n",
    "        for col in self._col_names:\n",
    "            if X[col].dtype == 'string':\n",
    "                X[col].fillna('-1', inplace=True)\n",
    "                X[col] = X[col].str.split(pat=',').apply(set).apply(list)\n",
    "            temp_series = X[col].explode()\n",
    "            temp_series = temp_series.replace({'true':'1', 'false':'0'}).fillna('-1').apply(pd.to_numeric, args=('coerce',))\n",
    "            temp_series = temp_series.groupby(temp_series.index).max()\n",
    "            temp_df = temp_df.merge(temp_series, left_index=True, right_index=True, how='outer')\n",
    "        temp_df = temp_df.fillna(0)\n",
    "        return temp_df\n",
    "\n",
    "    def get_feature_names(self):\n",
    "        return self._col_names\n",
    "\n",
    "class ListNuniqueTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self._col_names = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self._col_names = list(X.columns)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        print('Running ListNuniqueTransformer')\n",
    "        temp_df = pd.DataFrame(index=X.index)\n",
    "        for col in self._col_names:\n",
    "            if X[col].dtype == 'string':\n",
    "                X[col] = X[col].dropna().str.split(pat=',').apply(set).apply(list)\n",
    "            temp_series = X[col].explode()\n",
    "            temp_series = temp_series.groupby(temp_series.index).nunique()\n",
    "            temp_df = temp_df.merge(temp_series, left_index=True, right_index=True, how='outer')\n",
    "        temp_df = temp_df.fillna(0)\n",
    "        return temp_df\n",
    "\n",
    "    def get_feature_names(self):\n",
    "        return self._col_names\n",
    "\n",
    "class DescStatTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self._col_names = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        print('Running DescStatTransformer')\n",
    "        temp_df = pd.DataFrame(index=X.index)\n",
    "        for col in X.columns:\n",
    "            if X[col].dtype == 'string':\n",
    "                X[col].fillna('-1', inplace=True)\n",
    "                X[col] = X[col].str.split(pat=',').apply(set).apply(list)\n",
    "            temp_series = X[col].explode()\n",
    "            temp_series = temp_series.fillna('-1').apply(pd.to_numeric, args=('coerce',))\n",
    "            temp_series = temp_series.groupby(temp_series.index).agg(['min', 'max', 'mean', 'std', 'nunique'])\n",
    "            temp_series.columns = [f'{col}-{x}' for x in temp_series.columns]\n",
    "            temp_df = temp_df.merge(temp_series, left_index=True, right_index=True, how='outer')\n",
    "        temp_df = temp_df.fillna(0)\n",
    "        self._col_names = list(temp_df.columns)\n",
    "        return temp_df\n",
    "\n",
    "    def get_feature_names(self):\n",
    "        return self._col_names\n",
    "\n",
    "class MultilabelTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self._filler = 'ml_empty'\n",
    "        self._encoder = None\n",
    "        self._transformer = None\n",
    "        self._col_names = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        X = X.fillna(self._filler).str.split(pat=',').apply(set).apply(list)\n",
    "        self._encoder = MultiLabelBinarizer()\n",
    "        self._encoder.fit(X)\n",
    "        self._col_names = [X.name + '_' + x for x in self._encoder.classes_]\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        print('Running MultilabelTransformer')\n",
    "        X = X.fillna(self._filler).str.split(pat=',').apply(set).apply(list)\n",
    "        trans_array = self._encoder.transform(X)\n",
    "        df = pd.DataFrame(trans_array, columns=self._col_names, index=X.index)        \n",
    "        return df\n",
    "\n",
    "    def get_feature_names(self):\n",
    "        return self._col_names\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--output-data-dir', type=str, default=os.environ['SM_OUTPUT_DATA_DIR'])\n",
    "    parser.add_argument('--model-dir', type=str, default=os.environ['SM_MODEL_DIR'])\n",
    "    parser.add_argument('--train', type=str, default=os.environ['SM_CHANNEL_TRAIN'])\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    input_files = [os.path.join(args.train, file) for file in os.listdir(args.train)]\n",
    "    if len(input_files) == 0:\n",
    "        raise ValueError(('There are no files in {}.\\n' +\n",
    "                          'This usually indicates that the channel ({}) was incorrectly specified,\\n' +\n",
    "                          'the data specification in S3 was incorrectly specified or the role specified\\n' +\n",
    "                          'does not have permission to access the data.').format(args.train, \"train\"))\n",
    "    \n",
    "    file_types = [x.split('.')[-1] for x in os.listdir(args.train)]\n",
    "    type_list = list(set(file_types))\n",
    "    if len(type_list) != 1:\n",
    "           raise ValueError(('There are multiple file types or no files in {}.\\n' +\n",
    "                             'Please submit a single file or single file type.\\n' +\n",
    "                             'Accepted file types are csv, parquet, and json.').format(args.train))\n",
    "    else:\n",
    "        file_type = ''.join(type_list)\n",
    "        if file_type == 'csv':\n",
    "            raw_data = [ pd.read_csv(file) for file in input_files ]\n",
    "        elif file_type == 'parquet':\n",
    "            raw_data = [ pd.read_parquet(file) for file in input_files ]\n",
    "        elif file_type == 'json':\n",
    "            raw_data = [ pd.read_json(file) for file in input_files ]\n",
    "        else:\n",
    "            print('File type {} not accepted. Please use csv, parquet, or json'.format(file_type))\n",
    "    \n",
    "    train_data = pd.concat(raw_data)\n",
    "    \n",
    "    # print(train_data.head())\n",
    "    \n",
    "    preprocessor = ColumnTransformer([\n",
    "        ('drop_cols', 'drop', drop_cols),\n",
    "        ('truefalse', TrueFalseTransformer(), true_false),\n",
    "        ('onehot', OneHotTransformer(), one_hot),\n",
    "        ('dates', DateTransformer(), date_cols),\n",
    "        ('floats', FloatTransformer(), float_cols),\n",
    "        ('listmax', ListMaxTransformer(), max_of_list),\n",
    "        ('nunique', ListNuniqueTransformer(), count_unique),\n",
    "        ('descstats', DescStatTransformer(), desc_stat_cols),\n",
    "        ('multilabel', MultilabelTransformer(), 'multi_label')],\n",
    "        remainder='passthrough')\n",
    "\n",
    "    print('Preprocessing data')\n",
    "    preprocessor.fit(train_data)\n",
    "\n",
    "    print('Saving preprocessor joblib')\n",
    "    encoder_name = 'preprocessor.joblib'\n",
    "    joblib.dump(preprocessor, os.path.join(args.model_dir, encoder_name))\n",
    "    \n",
    "    print('Defining and saving selected feature names')\n",
    "    transform_col_list = drop_cols + true_false + one_hot + date_cols + float_cols + max_of_list + count_unique + desc_stat_cols + list_to_labels\n",
    "\n",
    "    step_list = ['truefalse',\n",
    "                 'onehot',\n",
    "                 'dates',\n",
    "                 'floats',\n",
    "                 'listmax',\n",
    "                 'nunique',\n",
    "                 'descstats',\n",
    "                 'multilabel']\n",
    "    \n",
    "    feature_names = []\n",
    "    \n",
    "    for step in step_list:\n",
    "        print(step)\n",
    "        item = preprocessor.named_transformers_[step].get_feature_names()\n",
    "        if type(item) == list:\n",
    "            feature_names = feature_names + item\n",
    "        elif type(item) == str:\n",
    "            feature_names = feature_names + [item]\n",
    "        else:\n",
    "            print('get_feature_names produced something other than a list or string')\n",
    "            print(type(item))\n",
    "            \n",
    "    remainder_cols = list(train_data.drop(transform_col_list, axis=1).columns)\n",
    "    feature_names = feature_names + remainder_cols\n",
    "    \n",
    "    joblib.dump(feature_names, os.path.join(args.model_dir, \"feature_names.joblib\"))\n",
    "\n",
    "    print(\"Selected features are: {}\".format(feature_names))\n",
    "    \n",
    "    \n",
    "def input_fn(input_data, content_type):\n",
    "    '''Parse input data payload\n",
    "    \n",
    "    Accepts csv, parquet, or json file types'''\n",
    "    \n",
    "    print('Running input function')\n",
    "    \n",
    "    if content_type == 'text/csv':\n",
    "        df = pd.read_csv(StringIO(input_data))\n",
    "        return df\n",
    "    elif content_type == 'application/x-parquet':\n",
    "        df = pd.read_parquet(input_data)\n",
    "    elif content_type == 'application/json':\n",
    "        df = pd.read_json(input_data)\n",
    "    else:\n",
    "        raise ValueError(\"{} not supported by script\".format(content_type))\n",
    "        \n",
    "def output_fn(prediction, accept):\n",
    "    '''Format prediction output.\n",
    "    \n",
    "    The default accept/content-type between containers for serial inference is JSON.\n",
    "    We also want to set the ContentType or mimetype as the same value as accept so the next\n",
    "    container can read the response payload correctly.\n",
    "    '''\n",
    "    \n",
    "    print('Running output function')\n",
    "    \n",
    "    if accept == 'application/json':\n",
    "        instances = []\n",
    "        for row in prediction.tolist():\n",
    "            instances.append({'features': row})\n",
    "            \n",
    "        json_output = {'instances': instances}\n",
    "        \n",
    "        return worker.Response(json.dumps(json_output), mimetype=accept)\n",
    "    elif accept == 'text/csv':\n",
    "        return worker.Response(encoders.encode(prediction, accept), mimetype=accept)\n",
    "    else:\n",
    "        raise RuntimeException('{} accept type is not supported by this script')\n",
    "        \n",
    "def predict_fn(input_data, model):\n",
    "    '''Preprocess input data\n",
    "    \n",
    "    The default predict_fn uses .predict(), but our model is a preprocessor\n",
    "    so we want to use .transform().\n",
    "    '''\n",
    "    \n",
    "    feat_names = joblib.load(os.path.join(model_dir, 'selected_feature_names.joblib'))\n",
    "    INPUT_FEATURES_SIZE = len(feat_names)\n",
    "    \n",
    "    print('Running predict_function')\n",
    "    \n",
    "    print('Input data shape at predict_fn: {}'.format(input_data.shape))\n",
    "    if input_data.shape[1] == INPUT_FEATURES_SIZE:\n",
    "        features = model.transform(input_data)\n",
    "        return features\n",
    "    elif input_data.shape[1] == INPUT_FEATURES_SIZE + 1:\n",
    "        # this assumes the target is the last column\n",
    "        features = model.transform(input_data.iloc[:, :-1])\n",
    "        # # This assumes the target is the first column\n",
    "        # features = model.transform(input_data.iloc[:, 1:])\n",
    "        return np.insert(features, 0, input_data[label_column], axis=1)\n",
    "        # What format should this be in? csv, json?\n",
    "        # Should I add the column names here?\n",
    "    \n",
    "def model_fn(model_dir):\n",
    "    '''Deserialize fitted model'''\n",
    "    \n",
    "    print('Running model function')\n",
    "    \n",
    "    preprocessor = joblib.load(os.path.join(model_dir, 'model.joblib'))\n",
    "    return preprocessor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076dd656-698f-48fd-bd7e-673eec30eeaa",
   "metadata": {},
   "source": [
    "## Train Preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c060463-a6ec-486a-9151-356c305c682a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "script_path = \"preprocess.py\"\n",
    "model_output_path = os.path.join('s3://', bucket, folder_name, \"components\")\n",
    "\n",
    "sklearn_transformer = SKLearn(\n",
    "    entry_point=script_path,\n",
    "    role=role,\n",
    "    output_path=model_output_path,\n",
    "    instance_type=\"ml.m5.large\",\n",
    "    sagemaker_session=None,\n",
    "    framework_version=\"1.0-1\",\n",
    "    py_version=\"py3\",\n",
    "    tags=tags\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ad9c7e75-df92-4319-a3ca-4965d0c19a01",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: sagemaker-scikit-learn-2023-02-22-17-27-36-005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-02-22 17:27:36 Starting - Starting the training job...\n",
      "2023-02-22 17:27:52 Starting - Preparing the instances for training......\n",
      "2023-02-22 17:29:04 Downloading - Downloading input data\n",
      "2023-02-22 17:29:04 Training - Downloading the training image.....\u001b[34m2023-02-22 17:29:49,303 sagemaker-containers INFO     Imported framework sagemaker_sklearn_container.training\u001b[0m\n",
      "\u001b[34m2023-02-22 17:29:49,307 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2023-02-22 17:29:49,315 sagemaker_sklearn_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2023-02-22 17:29:49,533 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2023-02-22 17:29:49,545 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2023-02-22 17:29:49,556 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2023-02-22 17:29:49,565 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_sklearn_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {},\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"sagemaker-scikit-learn-2023-02-22-17-27-36-005\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-707031497630/sagemaker-scikit-learn-2023-02-22-17-27-36-005/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"preprocess\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 2,\n",
      "    \"num_gpus\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.m5.large\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.m5.large\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"preprocess.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=preprocess.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.m5.large\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.large\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=preprocess\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_sklearn_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=2\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-707031497630/sagemaker-scikit-learn-2023-02-22-17-27-36-005/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_sklearn_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"sagemaker-scikit-learn-2023-02-22-17-27-36-005\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-707031497630/sagemaker-scikit-learn-2023-02-22-17-27-36-005/source/sourcedir.tar.gz\",\"module_name\":\"preprocess\",\"network_interface_name\":\"eth0\",\"num_cpus\":2,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.m5.large\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.large\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"preprocess.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/miniconda3/bin:/miniconda3/lib/python38.zip:/miniconda3/lib/python3.8:/miniconda3/lib/python3.8/lib-dynload:/miniconda3/lib/python3.8/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/miniconda3/bin/python preprocess.py\u001b[0m\n",
      "\u001b[34mCollecting pandas==1.3.5\n",
      "  Downloading pandas-1.3.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.5 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 11.5/11.5 MB 109.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.7.3 in /miniconda3/lib/python3.8/site-packages (from pandas==1.3.5) (2.8.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17.3 in /miniconda3/lib/python3.8/site-packages (from pandas==1.3.5) (1.19.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2017.3 in /miniconda3/lib/python3.8/site-packages (from pandas==1.3.5) (2022.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.5 in /miniconda3/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas==1.3.5) (1.15.0)\u001b[0m\n",
      "\u001b[34mInstalling collected packages: pandas\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 1.1.3\u001b[0m\n",
      "\u001b[34m    Uninstalling pandas-1.1.3:\n",
      "      Successfully uninstalled pandas-1.1.3\u001b[0m\n",
      "\u001b[34mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\u001b[0m\n",
      "\u001b[34msagemaker-sklearn-container 2.0 requires pandas==1.1.3, but you have pandas 1.3.5 which is incompatible.\u001b[0m\n",
      "\u001b[34mSuccessfully installed pandas-1.3.5\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip available: 22.3.1 -> 23.0.1\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\n",
      "2023-02-22 17:30:05 Uploading - Uploading generated training model\u001b[34mRequirement already satisfied: numpy in /miniconda3/lib/python3.8/site-packages (1.19.2)\u001b[0m\n",
      "\u001b[34mCollecting numpy\n",
      "  Downloading numpy-1.24.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 17.3/17.3 MB 90.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mInstalling collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.19.2\n",
      "    Uninstalling numpy-1.19.2:\n",
      "      Successfully uninstalled numpy-1.19.2\u001b[0m\n",
      "\u001b[34mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\u001b[0m\n",
      "\u001b[34msagemaker-sklearn-container 2.0 requires numpy==1.19.2, but you have numpy 1.24.2 which is incompatible.\u001b[0m\n",
      "\u001b[34msagemaker-sklearn-container 2.0 requires pandas==1.1.3, but you have pandas 1.3.5 which is incompatible.\u001b[0m\n",
      "\u001b[34mSuccessfully installed numpy-1.24.2\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip available: 22.3.1 -> 23.0.1\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow in /miniconda3/lib/python3.8/site-packages (1.0.0)\u001b[0m\n",
      "\u001b[34mCollecting pyarrow\n",
      "  Downloading pyarrow-11.0.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (35.0 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 35.0/35.0 MB 58.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.16.6 in /miniconda3/lib/python3.8/site-packages (from pyarrow) (1.24.2)\u001b[0m\n",
      "\u001b[34mInstalling collected packages: pyarrow\n",
      "  Attempting uninstall: pyarrow\n",
      "    Found existing installation: pyarrow 1.0.0\n",
      "    Uninstalling pyarrow-1.0.0:\u001b[0m\n",
      "\u001b[34m      Successfully uninstalled pyarrow-1.0.0\u001b[0m\n",
      "\u001b[34mSuccessfully installed pyarrow-11.0.0\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip available: 22.3.1 -> 23.0.1\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34mPreprocessing data\u001b[0m\n",
      "\u001b[34mRunning TrueFalseTransformer\u001b[0m\n",
      "\u001b[34mRunning OneHotTransformer\u001b[0m\n",
      "\u001b[34mRunning DateTransformer\u001b[0m\n",
      "\u001b[34mRunning FloatTransformer\u001b[0m\n",
      "\u001b[34mRunning ListMaxTransformer\u001b[0m\n",
      "\u001b[34mRunning ListNuniqueTransformer\u001b[0m\n",
      "\u001b[34mRunning DescStatTransformer\u001b[0m\n",
      "\u001b[34mRunning MultilabelTransformer\u001b[0m\n",
      "\u001b[34mSaving preprocessor joblib\u001b[0m\n",
      "\u001b[34mDefining and saving selected feature names\u001b[0m\n",
      "\u001b[34mtruefalse\u001b[0m\n",
      "\u001b[34monehot\u001b[0m\n",
      "\u001b[34mget_feature_names produced something other than a list or string\u001b[0m\n",
      "\u001b[34m<class 'numpy.ndarray'>\u001b[0m\n",
      "\u001b[34mdates\u001b[0m\n",
      "\u001b[34mfloats\u001b[0m\n",
      "\u001b[34mlistmax\u001b[0m\n",
      "\u001b[34mnunique\u001b[0m\n",
      "\u001b[34mdescstats\u001b[0m\n",
      "\u001b[34mmultilabel\u001b[0m\n",
      "\u001b[34mSelected features are: ['true_false', 'dates-month', 'dates-day_of_week', 'dates-hour', 'dates-day_of_month', 'dates-is_month_start', 'dates-is_month_end', 'floats', 'max_of_list', 'nunique_of_list', 'desc_stats-min', 'desc_stats-max', 'desc_stats-mean', 'desc_stats-std', 'desc_stats-nunique', 'multi_label_apple', 'multi_label_blueberry', 'multi_label_grape', 'multi_label_grapefruit', 'multi_label_ml_empty', 'multi_label_orange', 'multi_label_pineapple', 'multi_label_strawberry', 'Unnamed: 0', 'other']\u001b[0m\n",
      "\u001b[34m2023-02-22 17:30:02,545 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2023-02-22 17:30:15 Completed - Training job completed\n",
      "Training seconds: 97\n",
      "Billable seconds: 97\n"
     ]
    }
   ],
   "source": [
    "sklearn_transformer.fit({\"train\": train_input})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e33f884a-167d-43e7-96b4-96033c9c925d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "transformer_prefix = os.path.join(folder_name,\n",
    "                                  \"components\",\n",
    "                                  sklearn_transformer.latest_training_job.job_name,\n",
    "                                  \"output\",\n",
    "                                  \"model.tar.gz\")\n",
    "\n",
    "session.download_data(path='./', bucket=bucket, key_prefix=transformer_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3670ea8f-ff59-41ff-99c5-055fc4d5a58a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessor.joblib\n",
      "feature_names.joblib\n"
     ]
    }
   ],
   "source": [
    "!tar xvzf model.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a3bbc2e8-9e10-4c32-bcff-ab0ec8bcf2fb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['true_false', 'dates-month', 'dates-day_of_week', 'dates-hour', 'dates-day_of_month', 'dates-is_month_start', 'dates-is_month_end', 'floats', 'max_of_list', 'nunique_of_list', 'desc_stats-min', 'desc_stats-max', 'desc_stats-mean', 'desc_stats-std', 'desc_stats-nunique', 'multi_label_apple', 'multi_label_blueberry', 'multi_label_grape', 'multi_label_grapefruit', 'multi_label_ml_empty', 'multi_label_orange', 'multi_label_pineapple', 'multi_label_strawberry', 'Unnamed: 0', 'other']\n"
     ]
    }
   ],
   "source": [
    "feature_list = list(joblib.load(\"feature_names.joblib\"))\n",
    "print(feature_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d7829f42-73db-4f75-a02b-218794501406",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Can't get attribute 'TrueFalseTransformer' on <module '__main__'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-5f4b32b47db9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mjoblib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"preprocessor.joblib\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/joblib/numpy_pickle.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(filename, mmap_mode)\u001b[0m\n\u001b[1;32m    656\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mload_compatibility\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 658\u001b[0;31m                 \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_unpickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmmap_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    659\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/joblib/numpy_pickle.py\u001b[0m in \u001b[0;36m_unpickle\u001b[0;34m(fobj, filename, mmap_mode)\u001b[0m\n\u001b[1;32m    575\u001b[0m     \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 577\u001b[0;31m         \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    578\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat_mode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m             warnings.warn(\"The file '%s' has been generated with a \"\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/pickle.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1086\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mEOFError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1087\u001b[0m                 \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytes_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1088\u001b[0;31m                 \u001b[0mdispatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1089\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0m_Stop\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstopinst\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1090\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mstopinst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/pickle.py\u001b[0m in \u001b[0;36mload_stack_global\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1384\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mUnpicklingError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"STACK_GLOBAL requires str\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1385\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1386\u001b[0m     \u001b[0mdispatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mSTACK_GLOBAL\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_stack_global\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1387\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/pickle.py\u001b[0m in \u001b[0;36mfind_class\u001b[0;34m(self, module, name)\u001b[0m\n\u001b[1;32m   1426\u001b[0m         \u001b[0m__import__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1427\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproto\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1428\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0m_getattribute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1429\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1430\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/pickle.py\u001b[0m in \u001b[0;36m_getattribute\u001b[0;34m(obj, name)\u001b[0m\n\u001b[1;32m    297\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m             raise AttributeError(\"Can't get attribute {!r} on {!r}\"\n\u001b[0;32m--> 299\u001b[0;31m                                  .format(name, obj)) from None\n\u001b[0m\u001b[1;32m    300\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: Can't get attribute 'TrueFalseTransformer' on <module '__main__'>"
     ]
    }
   ],
   "source": [
    "joblib.load(\"preprocessor.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c8bede-022d-4e9e-a447-f4f032540bdb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
